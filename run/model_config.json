{"input_shape": 4, "output_shape": 1, "activation": "ReLU", "n_layers": 1, "n_nodes": [10], "L1_penalty": 0.001, "L2_penalty": 0.001, "use_batch_norm": true, "dropout_prob": 1}