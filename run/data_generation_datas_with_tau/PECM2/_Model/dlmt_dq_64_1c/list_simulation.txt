1.5171329087024788e-05

num_try : 242 | val_loss = 1.5171329087024788e-05 | val acc = 0.0027417384553700686
Time execution (tranning): 58.783821 seconds 
Time execution (load saved model): 0.001583 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 255
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

1.6402524615841684e-05

num_try : 196 | val_loss = 1.6402524615841684e-05 | val acc = 0.002992805792018771
Time execution (tranning): 72.082796 seconds 
Time execution (load saved model): 0.001525 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 310
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

1.6996673730318433e-05

num_try : 162 | val_loss = 1.6996673730318433e-05 | val acc = 0.00313090393319726
Time execution (tranning): 77.433602 seconds 
Time execution (load saved model): 0.001545 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 345
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

1.7537255807837937e-05

num_try : 161 | val_loss = 1.7537255807837937e-05 | val acc = 0.00302869058214128
Time execution (tranning): 56.009217 seconds 
Time execution (load saved model): 0.001534 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 256
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

1.7615185970498715e-05

num_try : 211 | val_loss = 1.7615185970498715e-05 | val acc = 0.003099223366007209
Time execution (tranning): 64.750503 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

1.7753226384229492e-05

num_try : 130 | val_loss = 1.7753226384229492e-05 | val acc = 0.003077080938965082
Time execution (tranning): 72.327951 seconds 
Time execution (load saved model): 0.001498 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 320
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

1.7815941955632297e-05

num_try : 194 | val_loss = 1.7815941955632297e-05 | val acc = 0.0030342992395162582
Time execution (tranning): 60.591533 seconds 
Time execution (load saved model): 0.001536 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

1.8099569060723298e-05

num_try : 210 | val_loss = 1.8099569060723298e-05 | val acc = 0.003185228444635868
Time execution (tranning): 86.432214 seconds 
Time execution (load saved model): 0.001533 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 372
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

1.825542311053141e-05

num_try : 195 | val_loss = 1.825542311053141e-05 | val acc = 0.00307137961499393
Time execution (tranning): 78.787685 seconds 
Time execution (load saved model): 0.001566 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 340
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

1.8416888306092005e-05

num_try : 144 | val_loss = 1.8416888306092005e-05 | val acc = 0.0031903048511594534
Time execution (tranning): 86.017605 seconds 
Time execution (load saved model): 0.001492 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 390
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

1.8820408840838354e-05

num_try : 243 | val_loss = 1.8820408840838354e-05 | val acc = 0.00295888539403677
Time execution (tranning): 61.846689 seconds 
Time execution (load saved model): 0.001782 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 271
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

1.882351856693276e-05

num_try : 147 | val_loss = 1.882351856693276e-05 | val acc = 0.0031464239582419395
Time execution (tranning): 71.039540 seconds 
Time execution (load saved model): 0.001498 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 312
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

1.91031526264851e-05

num_try : 181 | val_loss = 1.91031526264851e-05 | val acc = 0.003089507110416889
Time execution (tranning): 86.634094 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 378
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

1.9216861073800828e-05

num_try : 133 | val_loss = 1.9216861073800828e-05 | val acc = 0.0029944044072180986
Time execution (tranning): 75.267742 seconds 
Time execution (load saved model): 0.001503 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 337
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

1.9280833566881483e-05

num_try : 176 | val_loss = 1.9280833566881483e-05 | val acc = 0.0033518564887344837
Time execution (tranning): 62.632765 seconds 
Time execution (load saved model): 0.001497 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 285
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

1.9342720279382774e-05

num_try : 132 | val_loss = 1.9342720279382774e-05 | val acc = 0.003266621148213744
Time execution (tranning): 75.082917 seconds 
Time execution (load saved model): 0.001500 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 334
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

1.9382177815714384e-05

num_try : 68 | val_loss = 1.9382177815714384e-05 | val acc = 0.003386493306607008
Time execution (tranning): 67.141087 seconds 
Time execution (load saved model): 0.001499 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 304
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

1.9505167147144674e-05

num_try : 192 | val_loss = 1.9505167147144674e-05 | val acc = 0.003134224098175764
Time execution (tranning): 54.538958 seconds 
Time execution (load saved model): 0.001517 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 245
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.00621390104061e-05

num_try : 117 | val_loss = 2.00621390104061e-05 | val acc = 0.003173592034727335
Time execution (tranning): 50.473053 seconds 
Time execution (load saved model): 0.001513 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 225
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.0444949223019647e-05

num_try : 165 | val_loss = 2.0444949223019647e-05 | val acc = 0.0030770611483603716
Time execution (tranning): 43.584514 seconds 
Time execution (load saved model): 0.001526 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 194
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.045190009084763e-05

num_try : 129 | val_loss = 2.045190009084763e-05 | val acc = 0.0032718046568334103
Time execution (tranning): 51.813934 seconds 
Time execution (load saved model): 0.001492 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 234
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.0575980670400895e-05

num_try : 112 | val_loss = 2.0575980670400895e-05 | val acc = 0.0034498132299631834
Time execution (tranning): 67.102596 seconds 
Time execution (load saved model): 0.001509 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 312
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.070754246233264e-05

num_try : 100 | val_loss = 2.070754246233264e-05 | val acc = 0.003370476420968771
Time execution (tranning): 57.699480 seconds 
Time execution (load saved model): 0.001554 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 261
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.0722433655464558e-05

num_try : 64 | val_loss = 2.0722433655464558e-05 | val acc = 0.0034957474563270807
Time execution (tranning): 53.516846 seconds 
Time execution (load saved model): 0.001563 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 248
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.1156174516363534e-05

num_try : 114 | val_loss = 2.1156174516363534e-05 | val acc = 0.0036318572238087654
Time execution (tranning): 70.908047 seconds 
Time execution (load saved model): 0.001489 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 320
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.1230041711532975e-05

num_try : 84 | val_loss = 2.1230041711532975e-05 | val acc = 0.0035789308603852987
Time execution (tranning): 69.887068 seconds 
Time execution (load saved model): 0.001478 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 316
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.1278440290188882e-05

num_try : 193 | val_loss = 2.1278440290188882e-05 | val acc = 0.003214603289961815
Time execution (tranning): 68.643360 seconds 
Time execution (load saved model): 0.001507 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 294
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.1388273817137817e-05

num_try : 116 | val_loss = 2.1388273817137817e-05 | val acc = 0.003456070786342025
Time execution (tranning): 52.871922 seconds 
Time execution (load saved model): 0.001470 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 239
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.1472683438332752e-05

num_try : 85 | val_loss = 2.1472683438332752e-05 | val acc = 0.0034902668558061123
Time execution (tranning): 71.314562 seconds 
Time execution (load saved model): 0.001500 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 323
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.1536556632781868e-05

num_try : 427 | val_loss = 2.1536556632781868e-05 | val acc = 0.0033008938189595938
Time execution (tranning): 94.517928 seconds 
Time execution (load saved model): 0.001640 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 293
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.1711239496653435e-05

num_try : 179 | val_loss = 2.1711239496653435e-05 | val acc = 0.0032840126659721136
Time execution (tranning): 43.767379 seconds 
Time execution (load saved model): 0.001520 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 196
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.179928866098635e-05

num_try : 101 | val_loss = 2.179928866098635e-05 | val acc = 0.0034405833575874567
Time execution (tranning): 63.117851 seconds 
Time execution (load saved model): 0.001479 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.2128356304165207e-05

num_try : 69 | val_loss = 2.2128356304165207e-05 | val acc = 0.0035568808671087027
Time execution (tranning): 66.724116 seconds 
Time execution (load saved model): 0.001533 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 301
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.2188218754308763e-05

num_try : 113 | val_loss = 2.2188218754308763e-05 | val acc = 0.0035296957939863205
Time execution (tranning): 59.170926 seconds 
Time execution (load saved model): 0.001493 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 274
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.2370176375261507e-05

num_try : 226 | val_loss = 2.2370176375261507e-05 | val acc = 0.0034338736440986395
Time execution (tranning): 50.412167 seconds 
Time execution (load saved model): 0.001609 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 220
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.2370570550265258e-05

num_try : 208 | val_loss = 2.2370570550265258e-05 | val acc = 0.0034369744826108217
Time execution (tranning): 48.010542 seconds 
Time execution (load saved model): 0.001519 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 211
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.243053102574777e-05

num_try : 240 | val_loss = 2.243053102574777e-05 | val acc = 0.0031593122985213995
Time execution (tranning): 78.139006 seconds 
Time execution (load saved model): 0.001584 seconds 
Time execution (use saved model): 0.000174 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 341
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.2571777481061873e-05

num_try : 82 | val_loss = 2.2571777481061873e-05 | val acc = 0.003594005713239312
Time execution (tranning): 63.066224 seconds 
Time execution (load saved model): 0.001508 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 287
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.2869161184644327e-05

num_try : 241 | val_loss = 2.2869161184644327e-05 | val acc = 0.003337356960400939
Time execution (tranning): 52.194662 seconds 
Time execution (load saved model): 0.001525 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 228
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.3095659307728057e-05

num_try : 99 | val_loss = 2.3095659307728057e-05 | val acc = 0.0035918247886002064
Time execution (tranning): 62.721960 seconds 
Time execution (load saved model): 0.001593 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 281
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.3098285055311862e-05

num_try : 128 | val_loss = 2.3098285055311862e-05 | val acc = 0.003514033742249012
Time execution (tranning): 38.557409 seconds 
Time execution (load saved model): 0.001510 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 176
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.3336298254434952e-05

num_try : 178 | val_loss = 2.3336298254434952e-05 | val acc = 0.0031363163143396378
Time execution (tranning): 64.166496 seconds 
Time execution (load saved model): 0.001585 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.3495365712733474e-05

num_try : 282 | val_loss = 2.3495365712733474e-05 | val acc = 0.0036464938893914223
Time execution (tranning): 55.906889 seconds 
Time execution (load saved model): 0.001568 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 229
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.36361372662941e-05

num_try : 266 | val_loss = 2.36361372662941e-05 | val acc = 0.0035363524220883846
Time execution (tranning): 103.519055 seconds 
Time execution (load saved model): 0.001668 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 417
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.3919601189845708e-05

num_try : 298 | val_loss = 2.3919601189845708e-05 | val acc = 0.003739770036190748
Time execution (tranning): 48.799191 seconds 
Time execution (load saved model): 0.001557 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 202
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.409344608167885e-05

num_try : 96 | val_loss = 2.409344608167885e-05 | val acc = 0.003661388996988535
Time execution (tranning): 59.208672 seconds 
Time execution (load saved model): 0.001514 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 275
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.4263188206532503e-05

num_try : 164 | val_loss = 2.4263188206532503e-05 | val acc = 0.0035467904526740313
Time execution (tranning): 46.575554 seconds 
Time execution (load saved model): 0.001652 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.4312226923939306e-05

num_try : 81 | val_loss = 2.4312226923939306e-05 | val acc = 0.0035818838514387608
Time execution (tranning): 47.146644 seconds 
Time execution (load saved model): 0.001482 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 218
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.456353628076613e-05

num_try : 83 | val_loss = 2.456353628076613e-05 | val acc = 0.0035902876406908035
Time execution (tranning): 62.075313 seconds 
Time execution (load saved model): 0.001511 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 280
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.487046909664059e-05

num_try : 97 | val_loss = 2.487046909664059e-05 | val acc = 0.003643779084086418
Time execution (tranning): 72.550818 seconds 
Time execution (load saved model): 0.001504 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 332
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.49758675636258e-05

num_try : 66 | val_loss = 2.49758675636258e-05 | val acc = 0.0036863451823592186
Time execution (tranning): 56.720546 seconds 
Time execution (load saved model): 0.001502 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 257
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.512251081498107e-05

num_try : 32 | val_loss = 2.512251081498107e-05 | val acc = 0.004145749844610691
Time execution (tranning): 82.112809 seconds 
Time execution (load saved model): 0.001535 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 389
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.5189520965795963e-05

num_try : 80 | val_loss = 2.5189520965795963e-05 | val acc = 0.0036018723621964455
Time execution (tranning): 51.493593 seconds 
Time execution (load saved model): 0.001487 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 240
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.5284039402322378e-05

num_try : 209 | val_loss = 2.5284039402322378e-05 | val acc = 0.0037349991034716368
Time execution (tranning): 43.301911 seconds 
Time execution (load saved model): 0.001508 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 193
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.5325094611616805e-05

num_try : 67 | val_loss = 2.5325094611616805e-05 | val acc = 0.00368864624761045
Time execution (tranning): 76.381767 seconds 
Time execution (load saved model): 0.001497 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 346
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.5384356849826872e-05

num_try : 212 | val_loss = 2.5384356849826872e-05 | val acc = 0.0036936337128281593
Time execution (tranning): 45.791582 seconds 
Time execution (load saved model): 0.001713 seconds 
Time execution (use saved model): 0.000179 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 197
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.54589401447447e-05

num_try : 307 | val_loss = 2.54589401447447e-05 | val acc = 0.0036144552286714315
Time execution (tranning): 67.201135 seconds 
Time execution (load saved model): 0.001599 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.5559786627127325e-05

num_try : 180 | val_loss = 2.5559786627127325e-05 | val acc = 0.0036429520696401596
Time execution (tranning): 66.321456 seconds 
Time execution (load saved model): 0.001584 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 293
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.560387223638827e-05

num_try : 280 | val_loss = 2.560387223638827e-05 | val acc = 0.003994184546172619
Time execution (tranning): 80.425401 seconds 
Time execution (load saved model): 0.001540 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 341
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.5622765497246293e-05

num_try : 225 | val_loss = 2.5622765497246293e-05 | val acc = 0.003562642727047205
Time execution (tranning): 43.404801 seconds 
Time execution (load saved model): 0.001534 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 193
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.584195459348848e-05

num_try : 244 | val_loss = 2.584195459348848e-05 | val acc = 0.00366705353371799
Time execution (tranning): 57.351466 seconds 
Time execution (load saved model): 0.001512 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 245
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.5846085336524993e-05

num_try : 177 | val_loss = 2.5846085336524993e-05 | val acc = 0.0038308787625283003
Time execution (tranning): 56.401291 seconds 
Time execution (load saved model): 0.001576 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 253
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.609888935694471e-05

num_try : 290 | val_loss = 2.609888935694471e-05 | val acc = 0.0036523255985230207
Time execution (tranning): 46.448091 seconds 
Time execution (load saved model): 0.001651 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 191
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.6349193140049464e-05

num_try : 228 | val_loss = 2.6349193140049464e-05 | val acc = 0.0037509568501263857
Time execution (tranning): 51.097009 seconds 
Time execution (load saved model): 0.001509 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 217
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.645269640197512e-05

num_try : 338 | val_loss = 2.645269640197512e-05 | val acc = 0.003749716328456998
Time execution (tranning): 58.851712 seconds 
Time execution (load saved model): 0.001673 seconds 
Time execution (use saved model): 0.000182 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 218
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.6565569460217375e-05

num_try : 188 | val_loss = 2.6565569460217375e-05 | val acc = 0.003889198414981365
Time execution (tranning): 88.772957 seconds 
Time execution (load saved model): 0.001509 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 395
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.6600450510159135e-05

num_try : 16 | val_loss = 2.6600450510159135e-05 | val acc = 0.004123051185160875
Time execution (tranning): 46.603440 seconds 
Time execution (load saved model): 0.001623 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 220
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.6678176946006717e-05

num_try : 37 | val_loss = 2.6678176946006717e-05 | val acc = 0.0040533398278057575
Time execution (tranning): 48.448750 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 222
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.6768518218887038e-05

num_try : 274 | val_loss = 2.6768518218887038e-05 | val acc = 0.0037305611185729504
Time execution (tranning): 41.843510 seconds 
Time execution (load saved model): 0.001528 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 169
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.6795146659424063e-05

num_try : 115 | val_loss = 2.6795146659424063e-05 | val acc = 0.0037522749044001102
Time execution (tranning): 62.790150 seconds 
Time execution (load saved model): 0.001549 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 285
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.681213831237983e-05

num_try : 148 | val_loss = 2.681213831237983e-05 | val acc = 0.0038600543048232794
Time execution (tranning): 51.400829 seconds 
Time execution (load saved model): 0.001511 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 228
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.69988862783066e-05

num_try : 1 | val_loss = 2.69988862783066e-05 | val acc = 0.003955164458602667
Time execution (tranning): 69.011900 seconds 
Time execution (load saved model): 0.001462 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 328
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.7144390442117585e-05

num_try : 213 | val_loss = 2.7144390442117585e-05 | val acc = 0.0037533852737396955
Time execution (tranning): 47.316034 seconds 
Time execution (load saved model): 0.001561 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 198
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.7195573820790742e-05

num_try : 252 | val_loss = 2.7195573820790742e-05 | val acc = 0.003846627427265048
Time execution (tranning): 88.436420 seconds 
Time execution (load saved model): 0.001525 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 379
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.722236018598778e-05

num_try : 98 | val_loss = 2.722236018598778e-05 | val acc = 0.003842861158773303
Time execution (tranning): 54.012251 seconds 
Time execution (load saved model): 0.001551 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 240
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.7301088957756294e-05

num_try : 50 | val_loss = 2.7301088957756294e-05 | val acc = 0.004044089466333389
Time execution (tranning): 45.257472 seconds 
Time execution (load saved model): 0.001478 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 204
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.7350811869837343e-05

num_try : 131 | val_loss = 2.7350811869837343e-05 | val acc = 0.00377930561080575
Time execution (tranning): 33.653748 seconds 
Time execution (load saved model): 0.001504 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 149
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.738859177043196e-05

num_try : 21 | val_loss = 2.738859177043196e-05 | val acc = 0.004134499933570623
Time execution (tranning): 56.295992 seconds 
Time execution (load saved model): 0.001484 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.751409490883816e-05

num_try : 250 | val_loss = 2.751409490883816e-05 | val acc = 0.004076014272868633
Time execution (tranning): 48.419025 seconds 
Time execution (load saved model): 0.001526 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 203
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.7642193308565766e-05

num_try : 156 | val_loss = 2.7642193308565766e-05 | val acc = 0.004008227493613958
Time execution (tranning): 72.029322 seconds 
Time execution (load saved model): 0.001590 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 318
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.7724045030481647e-05

num_try : 34 | val_loss = 2.7724045030481647e-05 | val acc = 0.004219460766762495
Time execution (tranning): 53.777251 seconds 
Time execution (load saved model): 0.001478 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 246
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.774547403532779e-05

num_try : 275 | val_loss = 2.774547403532779e-05 | val acc = 0.0037600789219141006
Time execution (tranning): 50.112443 seconds 
Time execution (load saved model): 0.001618 seconds 
Time execution (use saved model): 0.000174 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.7801417381851934e-05

num_try : 17 | val_loss = 2.7801417381851934e-05 | val acc = 0.0040714624337852
Time execution (tranning): 46.418283 seconds 
Time execution (load saved model): 0.001538 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 220
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.7849059042637235e-05

num_try : 256 | val_loss = 2.7849059042637235e-05 | val acc = 0.0038325292989611626
Time execution (tranning): 58.111403 seconds 
Time execution (load saved model): 0.001642 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 241
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.7858466673933434e-05

num_try : 5 | val_loss = 2.7858466673933434e-05 | val acc = 0.004069061949849129
Time execution (tranning): 60.915472 seconds 
Time execution (load saved model): 0.001497 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 281
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.7913166013604497e-05

num_try : 371 | val_loss = 2.7913166013604497e-05 | val acc = 0.0037855333648622036
Time execution (tranning): 62.707622 seconds 
Time execution (load saved model): 0.001601 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 229
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.79954309007735e-05

num_try : 145 | val_loss = 2.79954309007735e-05 | val acc = 0.003815416945144534
Time execution (tranning): 58.009460 seconds 
Time execution (load saved model): 0.001499 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 264
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.8019966375723015e-05

num_try : 306 | val_loss = 2.8019966375723015e-05 | val acc = 0.00392094487324357
Time execution (tranning): 47.545521 seconds 
Time execution (load saved model): 0.001550 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 196
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.813351991790114e-05

num_try : 224 | val_loss = 2.813351991790114e-05 | val acc = 0.003904009237885475
Time execution (tranning): 48.136090 seconds 
Time execution (load saved model): 0.001525 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 211
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.828861186571885e-05

num_try : 234 | val_loss = 2.828861186571885e-05 | val acc = 0.004345125053077936
Time execution (tranning): 70.479316 seconds 
Time execution (load saved model): 0.001598 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 305
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.8311499627307057e-05

num_try : 248 | val_loss = 2.8311499627307057e-05 | val acc = 0.003943480551242828
Time execution (tranning): 71.325104 seconds 
Time execution (load saved model): 0.001546 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 320
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.8320013407210353e-05

num_try : 291 | val_loss = 2.8320013407210353e-05 | val acc = 0.0037791922222822905
Time execution (tranning): 52.939554 seconds 
Time execution (load saved model): 0.001554 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 220
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.8413105465006083e-05

num_try : 395 | val_loss = 2.8413105465006083e-05 | val acc = 0.003894351189956069
Time execution (tranning): 76.289907 seconds 
Time execution (load saved model): 0.001673 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 237
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.8448102821130305e-05

num_try : 19 | val_loss = 2.8448102821130305e-05 | val acc = 0.004170622676610947
Time execution (tranning): 42.900217 seconds 
Time execution (load saved model): 0.001477 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 198
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.847014991857577e-05

num_try : 163 | val_loss = 2.847014991857577e-05 | val acc = 0.0038551408797502518
Time execution (tranning): 40.266942 seconds 
Time execution (load saved model): 0.001514 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 178
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.864824935386423e-05

num_try : 2 | val_loss = 2.864824935386423e-05 | val acc = 0.004161926917731762
Time execution (tranning): 41.130566 seconds 
Time execution (load saved model): 0.001468 seconds 
Time execution (use saved model): 0.000158 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 190
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.8701048613584134e-05

num_try : 322 | val_loss = 2.8701048613584134e-05 | val acc = 0.0038980040699243546
Time execution (tranning): 51.420204 seconds 
Time execution (load saved model): 0.001586 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 194
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.8727458229695912e-05

num_try : 197 | val_loss = 2.8727458229695912e-05 | val acc = 0.003819066798314452
Time execution (tranning): 61.447256 seconds 
Time execution (load saved model): 0.001537 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 268
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.884747002099175e-05

num_try : 229 | val_loss = 2.884747002099175e-05 | val acc = 0.003863598918542266
Time execution (tranning): 46.735767 seconds 
Time execution (load saved model): 0.001538 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 201
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.8918255848111584e-05

num_try : 146 | val_loss = 2.8918255848111584e-05 | val acc = 0.003941610921174288
Time execution (tranning): 46.306165 seconds 
Time execution (load saved model): 0.001507 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.8928689171152656e-05

num_try : 235 | val_loss = 2.8928689171152656e-05 | val acc = 0.004238030407577753
Time execution (tranning): 69.149165 seconds 
Time execution (load saved model): 0.001583 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 300
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.896490565035492e-05

num_try : 354 | val_loss = 2.896490565035492e-05 | val acc = 0.0038679202552884817
Time execution (tranning): 45.445942 seconds 
Time execution (load saved model): 0.001575 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 168
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.9123274907760786e-05

num_try : 314 | val_loss = 2.9123274907760786e-05 | val acc = 0.004278665874153376
Time execution (tranning): 56.732452 seconds 
Time execution (load saved model): 0.001749 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 233
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.9215765644039492e-05

num_try : 258 | val_loss = 2.9215765644039492e-05 | val acc = 0.003950126469135284
Time execution (tranning): 49.800746 seconds 
Time execution (load saved model): 0.001538 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.9266428500704934e-05

num_try : 227 | val_loss = 2.9266428500704934e-05 | val acc = 0.0038635004311800003
Time execution (tranning): 35.472005 seconds 
Time execution (load saved model): 0.001705 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 154
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.926832930825185e-05

num_try : 138 | val_loss = 2.926832930825185e-05 | val acc = 0.004116615280508995
Time execution (tranning): 43.931043 seconds 
Time execution (load saved model): 0.001517 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 196
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.9339564898691607e-05

num_try : 53 | val_loss = 2.9339564898691607e-05 | val acc = 0.004246625117957592
Time execution (tranning): 47.355716 seconds 
Time execution (load saved model): 0.001500 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 216
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.9379060433711855e-05

num_try : 154 | val_loss = 2.9379060433711855e-05 | val acc = 0.004337580408900976
Time execution (tranning): 79.784984 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 354
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

2.9438367600960192e-05

num_try : 259 | val_loss = 2.9438367600960192e-05 | val acc = 0.003918868023902178
Time execution (tranning): 54.348530 seconds 
Time execution (load saved model): 0.001540 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 223
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

2.944294690678362e-05

num_try : 168 | val_loss = 2.944294690678362e-05 | val acc = 0.004110957030206919
Time execution (tranning): 60.471847 seconds 
Time execution (load saved model): 0.001498 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 277
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

2.9736565265920946e-05

num_try : 49 | val_loss = 2.9736565265920946e-05 | val acc = 0.004047010559588671
Time execution (tranning): 81.002705 seconds 
Time execution (load saved model): 0.001503 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 366
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

2.9800642805639655e-05

num_try : 20 | val_loss = 2.9800642805639655e-05 | val acc = 0.004213212989270687
Time execution (tranning): 43.097579 seconds 
Time execution (load saved model): 0.001468 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 198
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

2.993392823555041e-05

num_try : 245 | val_loss = 2.993392823555041e-05 | val acc = 0.003947028424590826
Time execution (tranning): 48.608505 seconds 
Time execution (load saved model): 0.001528 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 211
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

2.99401157462853e-05

num_try : 204 | val_loss = 2.99401157462853e-05 | val acc = 0.004026546608656645
Time execution (tranning): 44.683548 seconds 
Time execution (load saved model): 0.001612 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 186
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.0135608394630253e-05

num_try : 370 | val_loss = 3.0135608394630253e-05 | val acc = 0.0039819893427193165
Time execution (tranning): 46.607774 seconds 
Time execution (load saved model): 0.001899 seconds 
Time execution (use saved model): 0.000197 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 170
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.0168348894221707e-05

num_try : 52 | val_loss = 3.0168348894221707e-05 | val acc = 0.004270030651241541
Time execution (tranning): 60.406519 seconds 
Time execution (load saved model): 0.001472 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 275
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.0224413603718858e-05

num_try : 219 | val_loss = 3.0224413603718858e-05 | val acc = 0.004422677680850029
Time execution (tranning): 75.074432 seconds 
Time execution (load saved model): 0.001560 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 328
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.0240721425798256e-05

num_try : 33 | val_loss = 3.0240721425798256e-05 | val acc = 0.004242054186761379
Time execution (tranning): 48.644914 seconds 
Time execution (load saved model): 0.001533 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 225
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.0346855273819527e-05

num_try : 35 | val_loss = 3.0346855273819527e-05 | val acc = 0.004153703339397907
Time execution (tranning): 53.478321 seconds 
Time execution (load saved model): 0.001486 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 245
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.053782937058713e-05

num_try : 152 | val_loss = 3.053782937058713e-05 | val acc = 0.004623183980584145
Time execution (tranning): 67.300023 seconds 
Time execution (load saved model): 0.001518 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 303
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.0608573397330474e-05

num_try : 4 | val_loss = 3.0608573397330474e-05 | val acc = 0.004227971658110619
Time execution (tranning): 67.115967 seconds 
Time execution (load saved model): 0.001476 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 310
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.068319027079269e-05

num_try : 160 | val_loss = 3.068319027079269e-05 | val acc = 0.004062280524522066
Time execution (tranning): 63.803836 seconds 
Time execution (load saved model): 0.001505 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 293
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.088490717345849e-05

num_try : 304 | val_loss = 3.088490717345849e-05 | val acc = 0.004061623942106962
Time execution (tranning): 58.583128 seconds 
Time execution (load saved model): 0.001535 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 250
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.09024043235695e-05

num_try : 18 | val_loss = 3.09024043235695e-05 | val acc = 0.004296126309782267
Time execution (tranning): 39.319355 seconds 
Time execution (load saved model): 0.001484 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 181
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.1008257865323683e-05

num_try : 236 | val_loss = 3.1008257865323683e-05 | val acc = 0.004085961729288101
Time execution (tranning): 80.062608 seconds 
Time execution (load saved model): 0.001513 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 351
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.1163910607574506e-05

num_try : 51 | val_loss = 3.1163910607574506e-05 | val acc = 0.0043635587207973
Time execution (tranning): 45.809847 seconds 
Time execution (load saved model): 0.001501 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 208
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.144310805510031e-05

num_try : 405 | val_loss = 3.144310805510031e-05 | val acc = 0.0040618437342345715
Time execution (tranning): 88.551887 seconds 
Time execution (load saved model): 0.001751 seconds 
Time execution (use saved model): 0.000186 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 270
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.1676943908678364e-05

num_try : 324 | val_loss = 3.1676943908678364e-05 | val acc = 0.00413915840908885
Time execution (tranning): 75.046327 seconds 
Time execution (load saved model): 0.001660 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 269
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.1766175598022526e-05

num_try : 186 | val_loss = 3.1766175598022526e-05 | val acc = 0.004366763401776552
Time execution (tranning): 44.121681 seconds 
Time execution (load saved model): 0.001589 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 185
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.1823062599869444e-05

num_try : 0 | val_loss = 3.1823062599869444e-05 | val acc = 0.004225613083690405
Time execution (tranning): 59.387064 seconds 
Time execution (load saved model): 0.001840 seconds 
Time execution (use saved model): 0.000187 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 279
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.1872648032731374e-05

num_try : 323 | val_loss = 3.1872648032731374e-05 | val acc = 0.004046769812703133
Time execution (tranning): 56.298670 seconds 
Time execution (load saved model): 0.001758 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 209
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.195519930159208e-05

num_try : 169 | val_loss = 3.195519930159208e-05 | val acc = 0.004582889843732119
Time execution (tranning): 71.404927 seconds 
Time execution (load saved model): 0.001511 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 327
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.19660209788708e-05

num_try : 170 | val_loss = 3.19660209788708e-05 | val acc = 0.004305614158511162
Time execution (tranning): 41.258420 seconds 
Time execution (load saved model): 0.001497 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 183
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.2057105418061834e-05

num_try : 421 | val_loss = 3.2057105418061834e-05 | val acc = 0.003963594324886799
Time execution (tranning): 92.047304 seconds 
Time execution (load saved model): 0.001714 seconds 
Time execution (use saved model): 0.000179 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 287
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.2093062181957065e-05

num_try : 347 | val_loss = 3.2093062181957065e-05 | val acc = 0.004461065400391817
Time execution (tranning): 84.808885 seconds 
Time execution (load saved model): 0.001738 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 313
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.2240459222521165e-05

num_try : 149 | val_loss = 3.2240459222521165e-05 | val acc = 0.004048882517963648
Time execution (tranning): 37.782539 seconds 
Time execution (load saved model): 0.001548 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 167
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.2395874222856945e-05

num_try : 136 | val_loss = 3.2395874222856945e-05 | val acc = 0.004305158741772175
Time execution (tranning): 47.822604 seconds 
Time execution (load saved model): 0.001521 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 219
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.251233822084032e-05

num_try : 373 | val_loss = 3.251233822084032e-05 | val acc = 0.004095671698451042
Time execution (tranning): 60.441354 seconds 
Time execution (load saved model): 0.001726 seconds 
Time execution (use saved model): 0.000180 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 216
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.279618329543155e-05

num_try : 173 | val_loss = 3.279618329543155e-05 | val acc = 0.004578119143843651
Time execution (tranning): 82.079125 seconds 
Time execution (load saved model): 0.001511 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 358
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.286383049271535e-05

num_try : 355 | val_loss = 3.286383049271535e-05 | val acc = 0.00416962755843997
Time execution (tranning): 57.838279 seconds 
Time execution (load saved model): 0.001668 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 211
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.294650119642028e-05

num_try : 339 | val_loss = 3.294650119642028e-05 | val acc = 0.0041952054016292095
Time execution (tranning): 66.480498 seconds 
Time execution (load saved model): 0.001601 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 241
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.31909869419178e-05

num_try : 108 | val_loss = 3.31909869419178e-05 | val acc = 0.0046238680370152
Time execution (tranning): 70.429404 seconds 
Time execution (load saved model): 0.001707 seconds 
Time execution (use saved model): 0.000179 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 313
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.319800172903342e-05

num_try : 253 | val_loss = 3.319800172903342e-05 | val acc = 0.004065322689712048
Time execution (tranning): 76.685785 seconds 
Time execution (load saved model): 0.001813 seconds 
Time execution (use saved model): 0.000186 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 328
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.329182509332895e-05

num_try : 171 | val_loss = 3.329182509332895e-05 | val acc = 0.004839018452912569
Time execution (tranning): 63.971594 seconds 
Time execution (load saved model): 0.001520 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 285
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.3765781336114745e-05

num_try : 187 | val_loss = 3.3765781336114745e-05 | val acc = 0.004306546412408352
Time execution (tranning): 57.833593 seconds 
Time execution (load saved model): 0.001559 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 255
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.390911864698865e-05

num_try : 48 | val_loss = 3.390911864698865e-05 | val acc = 0.004478431772440672
Time execution (tranning): 69.194772 seconds 
Time execution (load saved model): 0.001532 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 323
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.434533296967857e-05

num_try : 205 | val_loss = 3.434533296967857e-05 | val acc = 0.0043045287020504475
Time execution (tranning): 66.891501 seconds 
Time execution (load saved model): 0.001516 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 291
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.444268684688723e-05

num_try : 418 | val_loss = 3.444268684688723e-05 | val acc = 0.004423986189067364
Time execution (tranning): 59.866537 seconds 
Time execution (load saved model): 0.001657 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 193
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.4812687881640156e-05

num_try : 155 | val_loss = 3.4812687881640156e-05 | val acc = 0.004281098488718271
Time execution (tranning): 88.127992 seconds 
Time execution (load saved model): 0.001495 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 388
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.48443759867223e-05

num_try : 419 | val_loss = 3.48443759867223e-05 | val acc = 0.004284978378564119
Time execution (tranning): 65.221337 seconds 
Time execution (load saved model): 0.001682 seconds 
Time execution (use saved model): 0.000179 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 216
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.4862012980738655e-05

num_try : 201 | val_loss = 3.4862012980738655e-05 | val acc = 0.004325198475271463
Time execution (tranning): 48.802403 seconds 
Time execution (load saved model): 0.001612 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 219
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.493672571494244e-05

num_try : 185 | val_loss = 3.493672571494244e-05 | val acc = 0.004689651075750589
Time execution (tranning): 74.495840 seconds 
Time execution (load saved model): 0.001546 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 339
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.500597493257373e-05

num_try : 3 | val_loss = 3.500597493257373e-05 | val acc = 0.004434741567820311
Time execution (tranning): 61.038454 seconds 
Time execution (load saved model): 0.001516 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 281
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.5123529269185386e-05

num_try : 172 | val_loss = 3.5123529269185386e-05 | val acc = 0.004442887380719185
Time execution (tranning): 72.214017 seconds 
Time execution (load saved model): 0.001506 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 322
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.513780844514258e-05

num_try : 104 | val_loss = 3.513780844514258e-05 | val acc = 0.0048206644132733345
Time execution (tranning): 61.920372 seconds 
Time execution (load saved model): 0.001534 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.5199728554289326e-05

num_try : 65 | val_loss = 3.5199728554289326e-05 | val acc = 0.004264775197952986
Time execution (tranning): 44.619777 seconds 
Time execution (load saved model): 0.001532 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 200
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.5205118329031394e-05

num_try : 36 | val_loss = 3.5205118329031394e-05 | val acc = 0.004529236350208521
Time execution (tranning): 77.656108 seconds 
Time execution (load saved model): 0.001464 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 352
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.520607813698007e-05

num_try : 435 | val_loss = 3.520607813698007e-05 | val acc = 0.004329020157456398
Time execution (tranning): 94.169517 seconds 
Time execution (load saved model): 0.001748 seconds 
Time execution (use saved model): 0.000189 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 275
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.5553173001972025e-05

num_try : 121 | val_loss = 3.5553173001972025e-05 | val acc = 0.00486738421022892
Time execution (tranning): 51.524425 seconds 
Time execution (load saved model): 0.001501 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1e-05
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 235
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.567401668988168e-05

num_try : 124 | val_loss = 3.567401668988168e-05 | val acc = 0.004863173235207796
Time execution (tranning): 57.557741 seconds 
Time execution (load saved model): 0.001486 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 253
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.57703158078948e-05

num_try : 402 | val_loss = 3.57703158078948e-05 | val acc = 0.004325147718191147
Time execution (tranning): 82.720093 seconds 
Time execution (load saved model): 0.001645 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 255
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.580399868951645e-05

num_try : 74 | val_loss = 3.580399868951645e-05 | val acc = 0.004927516914904118
Time execution (tranning): 71.572211 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 319
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.603981909691356e-05

num_try : 72 | val_loss = 3.603981909691356e-05 | val acc = 0.004731025546789169
Time execution (tranning): 58.013939 seconds 
Time execution (load saved model): 0.001485 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 263
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.606647638662252e-05

num_try : 140 | val_loss = 3.606647638662252e-05 | val acc = 0.004554484039545059
Time execution (tranning): 43.543887 seconds 
Time execution (load saved model): 0.001576 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 193
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.6101601726841184e-05

num_try : 316 | val_loss = 3.6101601726841184e-05 | val acc = 0.004496065899729729
Time execution (tranning): 60.340365 seconds 
Time execution (load saved model): 0.001543 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 246
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.6235088409739546e-05

num_try : 220 | val_loss = 3.6235088409739546e-05 | val acc = 0.004519721493124962
Time execution (tranning): 84.363063 seconds 
Time execution (load saved model): 0.001530 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 367
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.625934983574552e-05

num_try : 200 | val_loss = 3.625934983574552e-05 | val acc = 0.004490080755203962
Time execution (tranning): 49.043007 seconds 
Time execution (load saved model): 0.001558 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 220
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.626917445217259e-05

num_try : 139 | val_loss = 3.626917445217259e-05 | val acc = 0.0044237170368433
Time execution (tranning): 78.971287 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 354
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.628599046351155e-05

num_try : 384 | val_loss = 3.628599046351155e-05 | val acc = 0.004592415411025286
Time execution (tranning): 66.578840 seconds 
Time execution (load saved model): 0.001635 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 209
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.6289677664171906e-05

num_try : 403 | val_loss = 3.6289677664171906e-05 | val acc = 0.0044766562059521675
Time execution (tranning): 64.274493 seconds 
Time execution (load saved model): 0.001699 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 203
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.639642905909568e-05

num_try : 107 | val_loss = 3.639642905909568e-05 | val acc = 0.004795211832970381
Time execution (tranning): 56.584294 seconds 
Time execution (load saved model): 0.001494 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 253
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.643977928732056e-05

num_try : 77 | val_loss = 3.643977928732056e-05 | val acc = 0.004779861308634281
Time execution (tranning): 59.830548 seconds 
Time execution (load saved model): 0.001490 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 271
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.6624508356908334e-05

num_try : 122 | val_loss = 3.6624508356908334e-05 | val acc = 0.004788164049386978
Time execution (tranning): 42.943561 seconds 
Time execution (load saved model): 0.001514 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 185
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.671317477710545e-05

num_try : 346 | val_loss = 3.671317477710545e-05 | val acc = 0.004498360678553581
Time execution (tranning): 49.010278 seconds 
Time execution (load saved model): 0.001569 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 184
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.684534898638958e-05

num_try : 216 | val_loss = 3.684534898638958e-05 | val acc = 0.004507691133767366
Time execution (tranning): 51.348108 seconds 
Time execution (load saved model): 0.001536 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 231
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.6852080120297616e-05

num_try : 276 | val_loss = 3.6852080120297616e-05 | val acc = 0.004494480323046446
Time execution (tranning): 60.901657 seconds 
Time execution (load saved model): 0.001615 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 222
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.709357901243493e-05

num_try : 157 | val_loss = 3.709357901243493e-05 | val acc = 0.004514953121542931
Time execution (tranning): 48.294663 seconds 
Time execution (load saved model): 0.001518 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 213
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.7125591916264966e-05

num_try : 90 | val_loss = 3.7125591916264966e-05 | val acc = 0.004978932440280914
Time execution (tranning): 62.326749 seconds 
Time execution (load saved model): 0.001569 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 282
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.715999562700745e-05

num_try : 202 | val_loss = 3.715999562700745e-05 | val acc = 0.004571106750518084
Time execution (tranning): 34.552500 seconds 
Time execution (load saved model): 0.001508 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 149
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.717553176102228e-05

num_try : 315 | val_loss = 3.717553176102228e-05 | val acc = 0.004496966488659382
Time execution (tranning): 33.841229 seconds 
Time execution (load saved model): 0.001626 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 134
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.717959836649243e-05

num_try : 432 | val_loss = 3.717959836649243e-05 | val acc = 0.004548041615635157
Time execution (tranning): 92.936227 seconds 
Time execution (load saved model): 0.001718 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 295
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.723218811501283e-05

num_try : 105 | val_loss = 3.723218811501283e-05 | val acc = 0.004815352614969015
Time execution (tranning): 62.370619 seconds 
Time execution (load saved model): 0.001501 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 289
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.728893265360966e-05

num_try : 189 | val_loss = 3.728893265360966e-05 | val acc = 0.004533880390226841
Time execution (tranning): 63.756566 seconds 
Time execution (load saved model): 0.001503 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 285
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.731123397301417e-05

num_try : 89 | val_loss = 3.731123397301417e-05 | val acc = 0.004769300576299429
Time execution (tranning): 51.689288 seconds 
Time execution (load saved model): 0.001549 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 237
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.734468948096037e-05

num_try : 272 | val_loss = 3.734468948096037e-05 | val acc = 0.004579983185976744
Time execution (tranning): 66.723459 seconds 
Time execution (load saved model): 0.001536 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 277
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.748051807633601e-05

num_try : 106 | val_loss = 3.748051807633601e-05 | val acc = 0.005008671898394823
Time execution (tranning): 58.226125 seconds 
Time execution (load saved model): 0.001530 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 264
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.759149403776973e-05

num_try : 184 | val_loss = 3.759149403776973e-05 | val acc = 0.00465022400021553
Time execution (tranning): 55.051081 seconds 
Time execution (load saved model): 0.001509 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 250
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.7600913310598115e-05

num_try : 368 | val_loss = 3.7600913310598115e-05 | val acc = 0.0046368557959795
Time execution (tranning): 54.754539 seconds 
Time execution (load saved model): 0.001679 seconds 
Time execution (use saved model): 0.000179 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 206
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.7689203745685516e-05

num_try : 109 | val_loss = 3.7689203745685516e-05 | val acc = 0.005036033224314451
Time execution (tranning): 62.742589 seconds 
Time execution (load saved model): 0.001503 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 277
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.800420046900399e-05

num_try : 312 | val_loss = 3.800420046900399e-05 | val acc = 0.004658697638660669
Time execution (tranning): 45.411735 seconds 
Time execution (load saved model): 0.001566 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 189
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.803829153184779e-05

num_try : 434 | val_loss = 3.803829153184779e-05 | val acc = 0.004561317153275013
Time execution (tranning): 69.840079 seconds 
Time execution (load saved model): 0.001651 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 223
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.810316396993585e-05

num_try : 299 | val_loss = 3.810316396993585e-05 | val acc = 0.004544124472886324
Time execution (tranning): 66.188021 seconds 
Time execution (load saved model): 0.001562 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 266
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.821810030785855e-05

num_try : 120 | val_loss = 3.821810030785855e-05 | val acc = 0.004910243675112724
Time execution (tranning): 47.944439 seconds 
Time execution (load saved model): 0.001481 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 222
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.823722843662836e-05

num_try : 387 | val_loss = 3.823722843662836e-05 | val acc = 0.004604795016348362
Time execution (tranning): 80.935216 seconds 
Time execution (load saved model): 0.001709 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 254
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.862546836899128e-05

num_try : 237 | val_loss = 3.862546836899128e-05 | val acc = 0.004598579835146666
Time execution (tranning): 70.877887 seconds 
Time execution (load saved model): 0.001564 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 298
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

3.86448873905465e-05

num_try : 388 | val_loss = 3.86448873905465e-05 | val acc = 0.004681446123868227
Time execution (tranning): 83.391876 seconds 
Time execution (load saved model): 0.001698 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 262
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.869809901516419e-05

num_try : 283 | val_loss = 3.869809901516419e-05 | val acc = 0.004634654615074396
Time execution (tranning): 67.098600 seconds 
Time execution (load saved model): 0.001549 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 273
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.882728989992756e-05

num_try : 73 | val_loss = 3.882728989992756e-05 | val acc = 0.00474583962932229
Time execution (tranning): 90.786740 seconds 
Time execution (load saved model): 0.001497 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 420
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.8829519326100126e-05

num_try : 123 | val_loss = 3.8829519326100126e-05 | val acc = 0.004942500032484531
Time execution (tranning): 67.619967 seconds 
Time execution (load saved model): 0.001558 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 296
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.8858546249684876e-05

num_try : 362 | val_loss = 3.8858546249684876e-05 | val acc = 0.004709200467914343
Time execution (tranning): 61.350763 seconds 
Time execution (load saved model): 0.001590 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 229
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.8864151429152115e-05

num_try : 400 | val_loss = 3.8864151429152115e-05 | val acc = 0.004641433246433735
Time execution (tranning): 62.368548 seconds 
Time execution (load saved model): 0.001647 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.8869948402862066e-05

num_try : 232 | val_loss = 3.8869948402862066e-05 | val acc = 0.00472891004756093
Time execution (tranning): 36.534266 seconds 
Time execution (load saved model): 0.001584 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 163
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.90369483284303e-05

num_try : 288 | val_loss = 3.90369483284303e-05 | val acc = 0.004582641180604696
Time execution (tranning): 36.510971 seconds 
Time execution (load saved model): 0.001567 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 153
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

3.910823761543725e-05

num_try : 91 | val_loss = 3.910823761543725e-05 | val acc = 0.004975717514753342
Time execution (tranning): 44.764741 seconds 
Time execution (load saved model): 0.001510 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 200
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.915002729627304e-05

num_try : 218 | val_loss = 3.915002729627304e-05 | val acc = 0.004665356129407883
Time execution (tranning): 84.933141 seconds 
Time execution (load saved model): 0.001538 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 369
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.91913708881475e-05

num_try : 233 | val_loss = 3.91913708881475e-05 | val acc = 0.004699275828897953
Time execution (tranning): 49.796481 seconds 
Time execution (load saved model): 0.001544 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 221
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.922079908079468e-05

num_try : 257 | val_loss = 3.922079908079468e-05 | val acc = 0.0045780097134411335
Time execution (tranning): 49.749519 seconds 
Time execution (load saved model): 0.001670 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

3.982108348282054e-05

num_try : 436 | val_loss = 3.982108348282054e-05 | val acc = 0.004726147744804621
Time execution (tranning): 59.283316 seconds 
Time execution (load saved model): 0.001728 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 181
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.9825883359299045e-05

num_try : 203 | val_loss = 3.9825883359299045e-05 | val acc = 0.004716960247606039
Time execution (tranning): 50.345578 seconds 
Time execution (load saved model): 0.001584 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 214
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

3.9853641501395034e-05

num_try : 330 | val_loss = 3.9853641501395034e-05 | val acc = 0.0046341558918356895
Time execution (tranning): 48.920800 seconds 
Time execution (load saved model): 0.001615 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 180
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

3.991165584011469e-05

num_try : 260 | val_loss = 3.991165584011469e-05 | val acc = 0.004699038341641426
Time execution (tranning): 41.891232 seconds 
Time execution (load saved model): 0.001623 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 170
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

3.9923879376146944e-05

num_try : 217 | val_loss = 3.9923879376146944e-05 | val acc = 0.004715591203421354
Time execution (tranning): 58.151601 seconds 
Time execution (load saved model): 0.001535 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.0047172078629955e-05

num_try : 420 | val_loss = 4.0047172078629955e-05 | val acc = 0.004753334913402796
Time execution (tranning): 82.333410 seconds 
Time execution (load saved model): 0.001625 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 257
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.0137262258213016e-05

num_try : 340 | val_loss = 4.0137262258213016e-05 | val acc = 0.004720587283372879
Time execution (tranning): 45.609987 seconds 
Time execution (load saved model): 0.001620 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 168
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.021984157589031e-05

num_try : 249 | val_loss = 4.021984157589031e-05 | val acc = 0.004741355311125517
Time execution (tranning): 66.295937 seconds 
Time execution (load saved model): 0.001521 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 298
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.0266082505695523e-05

num_try : 321 | val_loss = 4.0266082505695523e-05 | val acc = 0.0045578088611364365
Time execution (tranning): 57.841013 seconds 
Time execution (load saved model): 0.001590 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 226
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.038366438180674e-05

num_try : 75 | val_loss = 4.038366438180674e-05 | val acc = 0.004851245786994696
Time execution (tranning): 73.867525 seconds 
Time execution (load saved model): 0.001473 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 329
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

4.0483295742888e-05

num_try : 386 | val_loss = 4.0483295742888e-05 | val acc = 0.004680456593632698
Time execution (tranning): 62.710880 seconds 
Time execution (load saved model): 0.001672 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 193
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.0562626527389514e-05

num_try : 292 | val_loss = 4.0562626527389514e-05 | val acc = 0.004772600252181292
Time execution (tranning): 47.146213 seconds 
Time execution (load saved model): 0.001546 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 195
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.059184997458942e-05

num_try : 416 | val_loss = 4.059184997458942e-05 | val acc = 0.004681652411818504
Time execution (tranning): 78.416740 seconds 
Time execution (load saved model): 0.001799 seconds 
Time execution (use saved model): 0.000186 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 262
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

4.068386493599974e-05

num_try : 277 | val_loss = 4.068386493599974e-05 | val acc = 0.004727480933070183
Time execution (tranning): 45.711544 seconds 
Time execution (load saved model): 0.001609 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 186
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.070492475875653e-05

num_try : 404 | val_loss = 4.070492475875653e-05 | val acc = 0.004816468805074692
Time execution (tranning): 45.846621 seconds 
Time execution (load saved model): 0.001643 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 146
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.0747128805378454e-05

num_try : 93 | val_loss = 4.0747128805378454e-05 | val acc = 0.004883281420916319
Time execution (tranning): 57.988961 seconds 
Time execution (load saved model): 0.001508 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.0784852535580286e-05

num_try : 353 | val_loss = 4.0784852535580286e-05 | val acc = 0.004796854220330715
Time execution (tranning): 43.578941 seconds 
Time execution (load saved model): 0.001642 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 165
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.114485658647027e-05

num_try : 137 | val_loss = 4.114485658647027e-05 | val acc = 0.004789948929101229
Time execution (tranning): 63.919173 seconds 
Time execution (load saved model): 0.001589 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 294
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.159714990237262e-05

num_try : 437 | val_loss = 4.159714990237262e-05 | val acc = 0.004781909286975861
Time execution (tranning): 70.938548 seconds 
Time execution (load saved model): 0.001700 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 216
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.1644670782261526e-05

num_try : 356 | val_loss = 4.1644670782261526e-05 | val acc = 0.004705122206360102
Time execution (tranning): 77.268337 seconds 
Time execution (load saved model): 0.001631 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 291
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.169871936028357e-05

num_try : 417 | val_loss = 4.169871936028357e-05 | val acc = 0.004749828949570656
Time execution (tranning): 78.031460 seconds 
Time execution (load saved model): 0.001656 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 256
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.1808320020209067e-05

num_try : 379 | val_loss = 4.1808320020209067e-05 | val acc = 0.004776120651513338
Time execution (tranning): 48.933297 seconds 
Time execution (load saved model): 0.001581 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 184
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

4.181029165920336e-05

num_try : 401 | val_loss = 4.181029165920336e-05 | val acc = 0.004748815204948187
Time execution (tranning): 77.316625 seconds 
Time execution (load saved model): 0.001689 seconds 
Time execution (use saved model): 0.000180 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 242
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.2054574078065346e-05

num_try : 389 | val_loss = 4.2054574078065346e-05 | val acc = 0.004846387077122927
Time execution (tranning): 70.202581 seconds 
Time execution (load saved model): 0.001651 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 229
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.206077006529085e-05

num_try : 285 | val_loss = 4.206077006529085e-05 | val acc = 0.0047715394757688046
Time execution (tranning): 48.762265 seconds 
Time execution (load saved model): 0.001698 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 201
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.209330210869666e-05

num_try : 251 | val_loss = 4.209330210869666e-05 | val acc = 0.004617259372025728
Time execution (tranning): 53.075699 seconds 
Time execution (load saved model): 0.001597 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 230
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

4.2385628039482983e-05

num_try : 320 | val_loss = 4.2385628039482983e-05 | val acc = 0.004801102448254824
Time execution (tranning): 54.108747 seconds 
Time execution (load saved model): 0.001651 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 210
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

4.241608228767291e-05

num_try : 221 | val_loss = 4.241608228767291e-05 | val acc = 0.004870271310210228
Time execution (tranning): 61.503247 seconds 
Time execution (load saved model): 0.001582 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 268
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.247737910191063e-05

num_try : 352 | val_loss = 4.247737910191063e-05 | val acc = 0.004761336371302605
Time execution (tranning): 53.877168 seconds 
Time execution (load saved model): 0.001577 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 201
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

4.257934524503071e-05

num_try : 289 | val_loss = 4.257934524503071e-05 | val acc = 0.004840590059757233
Time execution (tranning): 34.170445 seconds 
Time execution (load saved model): 0.001557 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 142
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.269398770702537e-05

num_try : 336 | val_loss = 4.269398770702537e-05 | val acc = 0.0048647779040038586
Time execution (tranning): 55.164920 seconds 
Time execution (load saved model): 0.001578 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

4.276553299860097e-05

num_try : 305 | val_loss = 4.276553299860097e-05 | val acc = 0.004832068458199501
Time execution (tranning): 37.380132 seconds 
Time execution (load saved model): 0.001618 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 155
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.2838315457629506e-05

num_try : 267 | val_loss = 4.2838315457629506e-05 | val acc = 0.004877092316746712
Time execution (tranning): 46.310921 seconds 
Time execution (load saved model): 0.001555 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 191
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

4.285148221242707e-05

num_try : 261 | val_loss = 4.285148221242707e-05 | val acc = 0.00479739997535944
Time execution (tranning): 62.338570 seconds 
Time execution (load saved model): 0.001549 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 256
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.295720929803792e-05

num_try : 341 | val_loss = 4.295720929803792e-05 | val acc = 0.004897509701550007
Time execution (tranning): 59.610410 seconds 
Time execution (load saved model): 0.001599 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 220
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.297965177102015e-05

num_try : 363 | val_loss = 4.297965177102015e-05 | val acc = 0.0048731304705142975
Time execution (tranning): 69.122574 seconds 
Time execution (load saved model): 0.001586 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 258
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

4.338941798778251e-05

num_try : 369 | val_loss = 4.338941798778251e-05 | val acc = 0.00475732795894146
Time execution (tranning): 62.217634 seconds 
Time execution (load saved model): 0.001644 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 233
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.344966684584506e-05

num_try : 378 | val_loss = 4.344966684584506e-05 | val acc = 0.005003409460186958
Time execution (tranning): 46.553838 seconds 
Time execution (load saved model): 0.001593 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 175
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.360970422567334e-05

num_try : 153 | val_loss = 4.360970422567334e-05 | val acc = 0.0049996464513242245
Time execution (tranning): 59.532595 seconds 
Time execution (load saved model): 0.001578 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 270
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.393180985061917e-05

num_try : 433 | val_loss = 4.393180985061917e-05 | val acc = 0.004894843325018883
Time execution (tranning): 66.773989 seconds 
Time execution (load saved model): 0.001648 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 215
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.39370512322057e-05

num_try : 410 | val_loss = 4.39370512322057e-05 | val acc = 0.005046594887971878
Time execution (tranning): 89.101370 seconds 
Time execution (load saved model): 0.001629 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 286
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.41241409862414e-05

num_try : 125 | val_loss = 4.41241409862414e-05 | val acc = 0.005112605169415474
Time execution (tranning): 60.280124 seconds 
Time execution (load saved model): 0.001479 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 266
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.4336841674521566e-05

num_try : 76 | val_loss = 4.4336841674521566e-05 | val acc = 0.005181692074984312
Time execution (tranning): 41.811709 seconds 
Time execution (load saved model): 0.001542 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 189
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.438062132976484e-05

num_try : 331 | val_loss = 4.438062132976484e-05 | val acc = 0.004977951757609844
Time execution (tranning): 40.125046 seconds 
Time execution (load saved model): 0.001587 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 147
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

4.4427414759411474e-05

num_try : 308 | val_loss = 4.4427414759411474e-05 | val acc = 0.004995196592062712
Time execution (tranning): 31.336741 seconds 
Time execution (load saved model): 0.001554 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 128
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.4487553968792784e-05

num_try : 264 | val_loss = 4.4487553968792784e-05 | val acc = 0.005017442163079977
Time execution (tranning): 82.119894 seconds 
Time execution (load saved model): 0.001535 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 348
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

4.503626390942373e-05

num_try : 385 | val_loss = 4.503626390942373e-05 | val acc = 0.004919327795505524
Time execution (tranning): 56.725922 seconds 
Time execution (load saved model): 0.001721 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 176
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.543346629361622e-05

num_try : 394 | val_loss = 4.543346629361622e-05 | val acc = 0.005062900949269533
Time execution (tranning): 79.107697 seconds 
Time execution (load saved model): 0.001813 seconds 
Time execution (use saved model): 0.000188 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 245
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.569361786707304e-05

num_try : 357 | val_loss = 4.569361786707304e-05 | val acc = 0.0049355835653841496
Time execution (tranning): 41.022816 seconds 
Time execution (load saved model): 0.001590 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 152
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.582048997690436e-05

num_try : 10 | val_loss = 4.582048997690436e-05 | val acc = 0.00567792821675539
Time execution (tranning): 56.855756 seconds 
Time execution (load saved model): 0.001474 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.586894458043389e-05

num_try : 397 | val_loss = 4.586894458043389e-05 | val acc = 0.004917234648019075
Time execution (tranning): 97.243832 seconds 
Time execution (load saved model): 0.001741 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 298
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.5870247413404285e-05

num_try : 268 | val_loss = 4.5870247413404285e-05 | val acc = 0.005130028817802668
Time execution (tranning): 45.930359 seconds 
Time execution (load saved model): 0.001544 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 187
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.605853326211218e-05

num_try : 313 | val_loss = 4.605853326211218e-05 | val acc = 0.004967099521309137
Time execution (tranning): 116.223137 seconds 
Time execution (load saved model): 0.001571 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 488
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.6106959489407016e-05

num_try : 442 | val_loss = 4.6106959489407016e-05 | val acc = 0.005092294421046972
Time execution (tranning): 102.831598 seconds 
Time execution (load saved model): 0.001668 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 320
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.629988587112166e-05

num_try : 92 | val_loss = 4.629988587112166e-05 | val acc = 0.005355557426810265
Time execution (tranning): 36.820904 seconds 
Time execution (load saved model): 0.001496 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 165
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.651514354918618e-05

num_try : 269 | val_loss = 4.651514354918618e-05 | val acc = 0.005014019086956978
Time execution (tranning): 101.718690 seconds 
Time execution (load saved model): 0.001549 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 414
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.66100279300008e-05

num_try : 293 | val_loss = 4.66100279300008e-05 | val acc = 0.004987102933228016
Time execution (tranning): 62.299458 seconds 
Time execution (load saved model): 0.001541 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 259
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.668820540246088e-05

num_try : 372 | val_loss = 4.668820540246088e-05 | val acc = 0.005082983989268541
Time execution (tranning): 55.668752 seconds 
Time execution (load saved model): 0.001671 seconds 
Time execution (use saved model): 0.000180 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.669476271374151e-05

num_try : 337 | val_loss = 4.669476271374151e-05 | val acc = 0.004972680937498808
Time execution (tranning): 46.192037 seconds 
Time execution (load saved model): 0.001599 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 175
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.725895531009883e-05

num_try : 380 | val_loss = 4.725895531009883e-05 | val acc = 0.0052223592065274715
Time execution (tranning): 90.984326 seconds 
Time execution (load saved model): 0.001598 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 337
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.744354278955143e-05

num_try : 273 | val_loss = 4.744354278955143e-05 | val acc = 0.005043834447860718
Time execution (tranning): 53.168688 seconds 
Time execution (load saved model): 0.001581 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 223
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.7507391354884024e-05

num_try : 141 | val_loss = 4.7507391354884024e-05 | val acc = 0.004994631744921207
Time execution (tranning): 53.678178 seconds 
Time execution (load saved model): 0.001516 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 230
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.789477723534219e-05

num_try : 44 | val_loss = 4.789477723534219e-05 | val acc = 0.005577581003308296
Time execution (tranning): 60.616612 seconds 
Time execution (load saved model): 0.001465 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.7920717915985735e-05

num_try : 281 | val_loss = 4.7920717915985735e-05 | val acc = 0.005176259204745293
Time execution (tranning): 94.844646 seconds 
Time execution (load saved model): 0.001557 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 404
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

4.829442397749517e-05

num_try : 300 | val_loss = 4.829442397749517e-05 | val acc = 0.005353795364499092
Time execution (tranning): 96.671976 seconds 
Time execution (load saved model): 0.001635 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 397
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.8298459660145454e-05

num_try : 58 | val_loss = 4.8298459660145454e-05 | val acc = 0.005718870088458061
Time execution (tranning): 58.222356 seconds 
Time execution (load saved model): 0.001536 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.830977093661204e-05

num_try : 325 | val_loss = 4.830977093661204e-05 | val acc = 0.005286271683871746
Time execution (tranning): 45.447894 seconds 
Time execution (load saved model): 0.001797 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 165
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

4.862230816797819e-05

num_try : 26 | val_loss = 4.862230816797819e-05 | val acc = 0.005820954218506813
Time execution (tranning): 61.622558 seconds 
Time execution (load saved model): 0.001458 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 275
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

4.8721199927967976e-05

num_try : 8 | val_loss = 4.8721199927967976e-05 | val acc = 0.005767006892710924
Time execution (tranning): 61.520773 seconds 
Time execution (load saved model): 0.001499 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 290
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

4.8964895904646254e-05

num_try : 28 | val_loss = 4.8964895904646254e-05 | val acc = 0.005852643400430679
Time execution (tranning): 59.836799 seconds 
Time execution (load saved model): 0.001472 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 267
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

4.902171349385753e-05

num_try : 309 | val_loss = 4.902171349385753e-05 | val acc = 0.005144269671291113
Time execution (tranning): 33.261081 seconds 
Time execution (load saved model): 0.001596 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 135
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

5.002143036108464e-05

num_try : 12 | val_loss = 5.002143036108464e-05 | val acc = 0.005840152036398649
Time execution (tranning): 54.486941 seconds 
Time execution (load saved model): 0.001459 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 249
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.0028879559249614e-05

num_try : 57 | val_loss = 5.0028879559249614e-05 | val acc = 0.005655275657773018
Time execution (tranning): 42.205974 seconds 
Time execution (load saved model): 0.001551 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 193
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.0240771597600544e-05

num_try : 56 | val_loss = 5.0240771597600544e-05 | val acc = 0.005807468667626381
Time execution (tranning): 48.957539 seconds 
Time execution (load saved model): 0.001484 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 228
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.049024613981601e-05

num_try : 24 | val_loss = 5.049024613981601e-05 | val acc = 0.0058453441597521305
Time execution (tranning): 37.893598 seconds 
Time execution (load saved model): 0.001501 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1e-05
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 179
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.094775064208079e-05

num_try : 42 | val_loss = 5.094775064208079e-05 | val acc = 0.005664435680955648
Time execution (tranning): 50.278841 seconds 
Time execution (load saved model): 0.001533 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 228
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

5.112867947900668e-05

num_try : 25 | val_loss = 5.112867947900668e-05 | val acc = 0.005776023957878351
Time execution (tranning): 64.906385 seconds 
Time execution (load saved model): 0.001466 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 305
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.142850372067187e-05

num_try : 88 | val_loss = 5.142850372067187e-05 | val acc = 0.005494755692780018
Time execution (tranning): 35.727197 seconds 
Time execution (load saved model): 0.001514 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 165
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.1495869993232194e-05

num_try : 43 | val_loss = 5.1495869993232194e-05 | val acc = 0.005851299036294222
Time execution (tranning): 62.361299 seconds 
Time execution (load saved model): 0.001550 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 278
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

5.2030746519449164e-05

num_try : 45 | val_loss = 5.2030746519449164e-05 | val acc = 0.005848316941410303
Time execution (tranning): 54.708356 seconds 
Time execution (load saved model): 0.001482 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 251
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

5.2209256900823674e-05

num_try : 11 | val_loss = 5.2209256900823674e-05 | val acc = 0.006090893875807524
Time execution (tranning): 39.702990 seconds 
Time execution (load saved model): 0.001473 seconds 
Time execution (use saved model): 0.000159 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1e-05
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 184
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

5.228615940723103e-05

num_try : 41 | val_loss = 5.228615940723103e-05 | val acc = 0.005894277710467577
Time execution (tranning): 48.600930 seconds 
Time execution (load saved model): 0.001486 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 229
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.2294892593636174e-05

num_try : 364 | val_loss = 5.2294892593636174e-05 | val acc = 0.005424209404736757
Time execution (tranning): 54.382690 seconds 
Time execution (load saved model): 0.001577 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 203
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.2429046481847765e-05

num_try : 40 | val_loss = 5.2429046481847765e-05 | val acc = 0.005805374588817358
Time execution (tranning): 41.905740 seconds 
Time execution (load saved model): 0.001472 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 196
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.2564312936738135e-05

num_try : 9 | val_loss = 5.2564312936738135e-05 | val acc = 0.00593724986538291
Time execution (tranning): 39.930805 seconds 
Time execution (load saved model): 0.001506 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 183
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.26463553251233e-05

num_try : 61 | val_loss = 5.26463553251233e-05 | val acc = 0.00578168872743845
Time execution (tranning): 49.227624 seconds 
Time execution (load saved model): 0.001487 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 225
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

5.2876801637467e-05

num_try : 29 | val_loss = 5.2876801637467e-05 | val acc = 0.0058809611946344376
Time execution (tranning): 42.021111 seconds 
Time execution (load saved model): 0.001537 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 189
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

5.3173132037045435e-05

num_try : 444 | val_loss = 5.3173132037045435e-05 | val acc = 0.005503181368112564
Time execution (tranning): 93.786800 seconds 
Time execution (load saved model): 0.001727 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 297
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.343260556401219e-05

num_try : 411 | val_loss = 5.343260556401219e-05 | val acc = 0.005495869088917971
Time execution (tranning): 64.164025 seconds 
Time execution (load saved model): 0.001721 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 206
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

5.345169731299393e-05

num_try : 376 | val_loss = 5.345169731299393e-05 | val acc = 0.005574177019298077
Time execution (tranning): 60.200948 seconds 
Time execution (load saved model): 0.001730 seconds 
Time execution (use saved model): 0.000180 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 217
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.3467513716896064e-05

num_try : 60 | val_loss = 5.3467513716896064e-05 | val acc = 0.005882987752556801
Time execution (tranning): 64.619835 seconds 
Time execution (load saved model): 0.001498 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 296
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.372469044232276e-05

num_try : 59 | val_loss = 5.372469044232276e-05 | val acc = 0.005931205581873655
Time execution (tranning): 43.402339 seconds 
Time execution (load saved model): 0.001620 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 194
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

5.372560321120545e-05

num_try : 428 | val_loss = 5.372560321120545e-05 | val acc = 0.005447644740343094
Time execution (tranning): 74.432823 seconds 
Time execution (load saved model): 0.001740 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 231
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.3866160160396246e-05

num_try : 344 | val_loss = 5.3866160160396246e-05 | val acc = 0.005400433670729399
Time execution (tranning): 54.526498 seconds 
Time execution (load saved model): 0.001634 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 208
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.504596781975124e-05

num_try : 392 | val_loss = 5.504596781975124e-05 | val acc = 0.005579973571002483
Time execution (tranning): 99.942461 seconds 
Time execution (load saved model): 0.001647 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 313
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.5214808307937345e-05

num_try : 426 | val_loss = 5.5214808307937345e-05 | val acc = 0.005717731546610594
Time execution (tranning): 78.015002 seconds 
Time execution (load saved model): 0.001712 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 244
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 1.0}
----------

5.610528969555162e-05

num_try : 13 | val_loss = 5.610528969555162e-05 | val acc = 0.005846744403243065
Time execution (tranning): 42.514917 seconds 
Time execution (load saved model): 0.001467 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 196
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

5.623268341878429e-05

num_try : 27 | val_loss = 5.623268341878429e-05 | val acc = 0.006143683101981878
Time execution (tranning): 45.114883 seconds 
Time execution (load saved model): 0.001517 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

5.6460743508068845e-05

num_try : 377 | val_loss = 5.6460743508068845e-05 | val acc = 0.0055649615824222565
Time execution (tranning): 44.686297 seconds 
Time execution (load saved model): 0.001649 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 164
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.6786657660268246e-05

num_try : 412 | val_loss = 5.6786657660268246e-05 | val acc = 0.005675842985510826
Time execution (tranning): 71.767079 seconds 
Time execution (load saved model): 0.001691 seconds 
Time execution (use saved model): 0.000182 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 231
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.7007637806236744e-05

num_try : 441 | val_loss = 5.7007637806236744e-05 | val acc = 0.0055963266640901566
Time execution (tranning): 66.871542 seconds 
Time execution (load saved model): 0.001656 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 213
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.7074899450526574e-05

num_try : 443 | val_loss = 5.7074899450526574e-05 | val acc = 0.005639008246362209
Time execution (tranning): 83.015875 seconds 
Time execution (load saved model): 0.001790 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 253
Criterion: ModifiedHuberLoss
with parameters: {'delta': 0.2, 'factor': 2.0}
----------

5.7180835065082646e-05

num_try : 440 | val_loss = 5.7180835065082646e-05 | val acc = 0.005558207631111145
Time execution (tranning): 49.261580 seconds 
Time execution (load saved model): 0.001664 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 158
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.7398088029003705e-05

num_try : 360 | val_loss = 5.7398088029003705e-05 | val acc = 0.005681492853909731
Time execution (tranning): 68.803807 seconds 
Time execution (load saved model): 0.001581 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 263
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.8175025551463477e-05

num_try : 332 | val_loss = 5.8175025551463477e-05 | val acc = 0.00560153741389513
Time execution (tranning): 66.107715 seconds 
Time execution (load saved model): 0.001656 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 244
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.8699568471638484e-05

num_try : 424 | val_loss = 5.8699568471638484e-05 | val acc = 0.0056400909088552
Time execution (tranning): 85.245506 seconds 
Time execution (load saved model): 0.001648 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 260
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.908052015001886e-05

num_try : 408 | val_loss = 5.908052015001886e-05 | val acc = 0.00563517352566123
Time execution (tranning): 64.478404 seconds 
Time execution (load saved model): 0.001641 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

5.9517704503377896e-05

num_try : 265 | val_loss = 5.9517704503377896e-05 | val acc = 0.005682198330760002
Time execution (tranning): 109.070712 seconds 
Time execution (load saved model): 0.001610 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 455
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.969206162262708e-05

num_try : 396 | val_loss = 5.969206162262708e-05 | val acc = 0.005772880744189024
Time execution (tranning): 63.849192 seconds 
Time execution (load saved model): 0.001646 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 197
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

5.978201326797716e-05

num_try : 409 | val_loss = 5.978201326797716e-05 | val acc = 0.005615852307528257
Time execution (tranning): 61.926552 seconds 
Time execution (load saved model): 0.001655 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 198
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.995109182549641e-05

num_try : 329 | val_loss = 5.995109182549641e-05 | val acc = 0.005696995183825493
Time execution (tranning): 67.470323 seconds 
Time execution (load saved model): 0.001577 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 252
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

5.9992025198880584e-05

num_try : 284 | val_loss = 5.9992025198880584e-05 | val acc = 0.005716725718230009
Time execution (tranning): 50.401268 seconds 
Time execution (load saved model): 0.001604 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 204
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

6.001872468914371e-05

num_try : 328 | val_loss = 6.001872468914371e-05 | val acc = 0.005666004493832588
Time execution (tranning): 55.445658 seconds 
Time execution (load saved model): 0.001667 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 208
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

6.05076316423947e-05

num_try : 381 | val_loss = 6.05076316423947e-05 | val acc = 0.005648027174174786
Time execution (tranning): 63.466459 seconds 
Time execution (load saved model): 0.001566 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 236
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.263326686166693e-05

num_try : 345 | val_loss = 6.263326686166693e-05 | val acc = 0.005785808432847261
Time execution (tranning): 47.174730 seconds 
Time execution (load saved model): 0.001585 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 181
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

6.31861638976261e-05

num_try : 297 | val_loss = 6.31861638976261e-05 | val acc = 0.005760132800787687
Time execution (tranning): 40.292514 seconds 
Time execution (load saved model): 0.001541 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 170
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

6.369909897330217e-05

num_try : 393 | val_loss = 6.369909897330217e-05 | val acc = 0.005868151783943176
Time execution (tranning): 61.231002 seconds 
Time execution (load saved model): 0.001817 seconds 
Time execution (use saved model): 0.000192 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 195
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

6.387337525666225e-05

num_try : 296 | val_loss = 6.387337525666225e-05 | val acc = 0.005800191313028336
Time execution (tranning): 68.339537 seconds 
Time execution (load saved model): 0.001551 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 291
Criterion: LogCoshLoss
with parameters: {'factor': 1.0}
----------

6.39346799289342e-05

num_try : 301 | val_loss = 6.39346799289342e-05 | val acc = 0.005814803298562765
Time execution (tranning): 58.495177 seconds 
Time execution (load saved model): 0.001553 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 243
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.430195149732754e-05

num_try : 333 | val_loss = 6.430195149732754e-05 | val acc = 0.005814839620143175
Time execution (tranning): 57.966036 seconds 
Time execution (load saved model): 0.001644 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 214
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.508322941954247e-05

num_try : 349 | val_loss = 6.508322941954247e-05 | val acc = 0.005856513977050781
Time execution (tranning): 56.412435 seconds 
Time execution (load saved model): 0.001989 seconds 
Time execution (use saved model): 0.000203 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 208
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.58489595662104e-05

num_try : 365 | val_loss = 6.58489595662104e-05 | val acc = 0.0058321584947407246
Time execution (tranning): 41.802384 seconds 
Time execution (load saved model): 0.001587 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 155
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.595660859602503e-05

num_try : 348 | val_loss = 6.595660859602503e-05 | val acc = 0.005938359536230564
Time execution (tranning): 47.169850 seconds 
Time execution (load saved model): 0.001595 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 176
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 1.0}
----------

6.630980067711789e-05

num_try : 425 | val_loss = 6.630980067711789e-05 | val acc = 0.006001764442771673
Time execution (tranning): 74.492708 seconds 
Time execution (load saved model): 0.001706 seconds 
Time execution (use saved model): 0.000183 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 236
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

6.686858119792305e-05

num_try : 413 | val_loss = 6.686858119792305e-05 | val acc = 0.005989041179418564
Time execution (tranning): 89.245515 seconds 
Time execution (load saved model): 0.001724 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 265
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.83738438237924e-05

num_try : 317 | val_loss = 6.83738438237924e-05 | val acc = 0.005831881891936064
Time execution (tranning): 79.881070 seconds 
Time execution (load saved model): 0.001662 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 329
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

6.918378843693063e-05

num_try : 429 | val_loss = 6.918378843693063e-05 | val acc = 0.0061765736900269985
Time execution (tranning): 66.622663 seconds 
Time execution (load saved model): 0.001642 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

7.390586033579893e-05

num_try : 361 | val_loss = 7.390586033579893e-05 | val acc = 0.00621704151853919
Time execution (tranning): 37.751442 seconds 
Time execution (load saved model): 0.001607 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=LogCoshLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 142
Criterion: LogCoshLoss
with parameters: {'factor': 1.8}
----------

7.477640720026102e-05

num_try : 445 | val_loss = 7.477640720026102e-05 | val acc = 0.00617809547111392
Time execution (tranning): 71.197029 seconds 
Time execution (load saved model): 0.001658 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ModifiedHuberLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 228
Criterion: ModifiedHuberLoss
with parameters: {'delta': 1.0, 'factor': 2.0}
----------

0.0012406988441944122

num_try : 438 | val_loss = 0.0012406988441944122 | val acc = 0.002471256535500288
Time execution (tranning): 92.331746 seconds 
Time execution (load saved model): 0.001730 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 287
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.001265678321942687

num_try : 390 | val_loss = 0.001265678321942687 | val acc = 0.002518398454412818
Time execution (tranning): 93.339500 seconds 
Time execution (load saved model): 0.001690 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 309
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0014024565555155277

num_try : 406 | val_loss = 0.0014024565555155277 | val acc = 0.0027935586404055357
Time execution (tranning): 95.105031 seconds 
Time execution (load saved model): 0.001659 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 309
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0014052619505673647

num_try : 342 | val_loss = 0.0014052619505673647 | val acc = 0.002797686494886875
Time execution (tranning): 74.157363 seconds 
Time execution (load saved model): 0.001648 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 281
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0014990480523556472

num_try : 374 | val_loss = 0.0014990480523556472 | val acc = 0.002983054146170616
Time execution (tranning): 61.758731 seconds 
Time execution (load saved model): 0.001722 seconds 
Time execution (use saved model): 0.000182 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 234
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.001549729099497199

num_try : 294 | val_loss = 0.001549729099497199 | val acc = 0.0030854246579110622
Time execution (tranning): 62.167204 seconds 
Time execution (load saved model): 0.001612 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 267
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0015528713632375001

num_try : 262 | val_loss = 0.0015528713632375001 | val acc = 0.0030914449598640203
Time execution (tranning): 64.148890 seconds 
Time execution (load saved model): 0.001617 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 269
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0015541255474090577

num_try : 150 | val_loss = 0.0015541255474090577 | val acc = 0.0030930815264582634
Time execution (tranning): 44.710801 seconds 
Time execution (load saved model): 0.001515 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0015554608311504126

num_try : 278 | val_loss = 0.0015554608311504126 | val acc = 0.0030960773583501577
Time execution (tranning): 50.312549 seconds 
Time execution (load saved model): 0.001646 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 211
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0015560096129775048

num_try : 358 | val_loss = 0.0015560096129775048 | val acc = 0.0030971572268754244
Time execution (tranning): 53.844230 seconds 
Time execution (load saved model): 0.001661 seconds 
Time execution (use saved model): 0.000179 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 202
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0015788127295672894

num_try : 430 | val_loss = 0.0015788127295672894 | val acc = 0.0031447038054466248
Time execution (tranning): 103.738008 seconds 
Time execution (load saved model): 0.001711 seconds 
Time execution (use saved model): 0.000182 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 318
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0016112976800650358

num_try : 134 | val_loss = 0.0016112976800650358 | val acc = 0.003208090318366885
Time execution (tranning): 35.609601 seconds 
Time execution (load saved model): 0.001505 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 164
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.001620427668094635

num_try : 182 | val_loss = 0.001620427668094635 | val acc = 0.003227842506021261
Time execution (tranning): 51.552445 seconds 
Time execution (load saved model): 0.001528 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 238
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0016274462826550006

num_try : 310 | val_loss = 0.0016274462826550006 | val acc = 0.0032391997519880533
Time execution (tranning): 58.099114 seconds 
Time execution (load saved model): 0.001549 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 246
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0016455059312283993

num_try : 446 | val_loss = 0.0016455059312283993 | val acc = 0.0032777555752545595
Time execution (tranning): 103.000926 seconds 
Time execution (load saved model): 0.001692 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 316
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0016612184140831232

num_try : 398 | val_loss = 0.0016612184140831232 | val acc = 0.003308097133412957
Time execution (tranning): 99.180507 seconds 
Time execution (load saved model): 0.001758 seconds 
Time execution (use saved model): 0.000182 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 317
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0016941381152719259

num_try : 326 | val_loss = 0.0016941381152719259 | val acc = 0.003369466168805957
Time execution (tranning): 54.916475 seconds 
Time execution (load saved model): 0.001605 seconds 
Time execution (use saved model): 0.000174 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 212
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0016970652900636196

num_try : 230 | val_loss = 0.0016970652900636196 | val acc = 0.0033763861283659935
Time execution (tranning): 65.659534 seconds 
Time execution (load saved model): 0.001538 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 294
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.001701618293300271

num_try : 102 | val_loss = 0.001701618293300271 | val acc = 0.003386378986760974
Time execution (tranning): 52.074583 seconds 
Time execution (load saved model): 0.001503 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 244
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017089683655649424

num_try : 198 | val_loss = 0.0017089683655649424 | val acc = 0.0034003155305981636
Time execution (tranning): 46.917226 seconds 
Time execution (load saved model): 0.001596 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 210
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017179730720818043

num_try : 86 | val_loss = 0.0017179730720818043 | val acc = 0.0034166055265814066
Time execution (tranning): 47.104811 seconds 
Time execution (load saved model): 0.001545 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 221
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017193074803799392

num_try : 166 | val_loss = 0.0017193074803799392 | val acc = 0.003423078916966915
Time execution (tranning): 61.597484 seconds 
Time execution (load saved model): 0.001517 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 286
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017566577065736056

num_try : 414 | val_loss = 0.0017566577065736056 | val acc = 0.003498923731967807
Time execution (tranning): 89.240168 seconds 
Time execution (load saved model): 0.001649 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 284
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017688345070928336

num_try : 118 | val_loss = 0.0017688345070928336 | val acc = 0.0035204968880861998
Time execution (tranning): 41.720222 seconds 
Time execution (load saved model): 0.001556 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 192
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017752652801573277

num_try : 70 | val_loss = 0.0017752652801573277 | val acc = 0.0035314676351845264
Time execution (tranning): 39.220433 seconds 
Time execution (load saved model): 0.001483 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 184
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0017846717964857817

num_try : 214 | val_loss = 0.0017846717964857817 | val acc = 0.0035529264714568853
Time execution (tranning): 52.737121 seconds 
Time execution (load saved model): 0.001508 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 232
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0018459307868033648

num_try : 246 | val_loss = 0.0018459307868033648 | val acc = 0.003672519000247121
Time execution (tranning): 59.440307 seconds 
Time execution (load saved model): 0.001584 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 269
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0019177894666790962

num_try : 22 | val_loss = 0.0019177894666790962 | val acc = 0.003814602503553033
Time execution (tranning): 53.861142 seconds 
Time execution (load saved model): 0.001471 seconds 
Time execution (use saved model): 0.000160 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 257
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0019374813325703144

num_try : 350 | val_loss = 0.0019374813325703144 | val acc = 0.003851044224575162
Time execution (tranning): 54.508682 seconds 
Time execution (load saved model): 0.001565 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0019520661234855652

num_try : 6 | val_loss = 0.0019520661234855652 | val acc = 0.0038818963803350925
Time execution (tranning): 33.140600 seconds 
Time execution (load saved model): 0.001497 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 158
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002013667169958353

num_try : 422 | val_loss = 0.002013667169958353 | val acc = 0.003999440930783749
Time execution (tranning): 66.574847 seconds 
Time execution (load saved model): 0.001626 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 207
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0020434072241187094

num_try : 38 | val_loss = 0.0020434072241187094 | val acc = 0.004061298444867134
Time execution (tranning): 52.326390 seconds 
Time execution (load saved model): 0.001472 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 249
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002053866358473897

num_try : 382 | val_loss = 0.002053866358473897 | val acc = 0.004081694409251213
Time execution (tranning): 46.517972 seconds 
Time execution (load saved model): 0.001689 seconds 
Time execution (use saved model): 0.000182 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 179
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002059775907546282

num_try : 366 | val_loss = 0.002059775907546282 | val acc = 0.00409462908282876
Time execution (tranning): 46.583721 seconds 
Time execution (load saved model): 0.001617 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 179
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002104647699743509

num_try : 334 | val_loss = 0.002104647699743509 | val acc = 0.0041862977668643
Time execution (tranning): 51.768375 seconds 
Time execution (load saved model): 0.001576 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 202
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0021047259587794543

num_try : 190 | val_loss = 0.0021047259587794543 | val acc = 0.004189710132777691
Time execution (tranning): 66.546065 seconds 
Time execution (load saved model): 0.001520 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 309
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.00210755905136466

num_try : 238 | val_loss = 0.00210755905136466 | val acc = 0.004191093612462282
Time execution (tranning): 72.471700 seconds 
Time execution (load saved model): 0.001511 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 324
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002115098312497139

num_try : 54 | val_loss = 0.002115098312497139 | val acc = 0.004207197111099958
Time execution (tranning): 39.777757 seconds 
Time execution (load saved model): 0.001489 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 188
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0021479593962430953

num_try : 318 | val_loss = 0.0021479593962430953 | val acc = 0.0042704129591584206
Time execution (tranning): 61.190951 seconds 
Time execution (load saved model): 0.001516 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 249
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002159696854650974

num_try : 142 | val_loss = 0.002159696854650974 | val acc = 0.004299077671021223
Time execution (tranning): 36.539109 seconds 
Time execution (load saved model): 0.001492 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 168
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002165390970185399

num_try : 286 | val_loss = 0.002165390970185399 | val acc = 0.004304495640099049
Time execution (tranning): 46.711804 seconds 
Time execution (load saved model): 0.001559 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 199
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.00216810068115592

num_try : 302 | val_loss = 0.00216810068115592 | val acc = 0.004312421660870314
Time execution (tranning): 62.689515 seconds 
Time execution (load saved model): 0.001631 seconds 
Time execution (use saved model): 0.000173 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 265
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002190667716786265

num_try : 206 | val_loss = 0.002190667716786265 | val acc = 0.00435645692050457
Time execution (tranning): 41.223900 seconds 
Time execution (load saved model): 0.001568 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 186
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0021920551639050246

num_try : 174 | val_loss = 0.0021920551639050246 | val acc = 0.0043626404367387295
Time execution (tranning): 72.365529 seconds 
Time execution (load saved model): 0.001527 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 332
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002193848481401801

num_try : 222 | val_loss = 0.002193848481401801 | val acc = 0.004366049077361822
Time execution (tranning): 67.687248 seconds 
Time execution (load saved model): 0.001546 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 302
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002206775853410363

num_try : 254 | val_loss = 0.002206775853410363 | val acc = 0.0043871039524674416
Time execution (tranning): 46.986301 seconds 
Time execution (load saved model): 0.001525 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 213
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002233243603259325

num_try : 126 | val_loss = 0.002233243603259325 | val acc = 0.004442969802767038
Time execution (tranning): 46.652788 seconds 
Time execution (load saved model): 0.001526 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 215
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0022546555660665035

num_try : 110 | val_loss = 0.0022546555660665035 | val acc = 0.004483591765165329
Time execution (tranning): 41.026785 seconds 
Time execution (load saved model): 0.001506 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 190
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0022833441942930223

num_try : 158 | val_loss = 0.0022833441942930223 | val acc = 0.0045364233665168285
Time execution (tranning): 33.224378 seconds 
Time execution (load saved model): 0.001542 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 152
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0023130187205970287

num_try : 270 | val_loss = 0.0023130187205970287 | val acc = 0.004599260166287422
Time execution (tranning): 42.620051 seconds 
Time execution (load saved model): 0.001546 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 178
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002351437546312809

num_try : 94 | val_loss = 0.002351437546312809 | val acc = 0.00467755738645792
Time execution (tranning): 42.638814 seconds 
Time execution (load saved model): 0.001486 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 200
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0024733281787484886

num_try : 78 | val_loss = 0.0024733281787484886 | val acc = 0.004914763383567333
Time execution (tranning): 40.072718 seconds 
Time execution (load saved model): 0.001504 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 186
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0026232269033789634

num_try : 375 | val_loss = 0.0026232269033789634 | val acc = 0.0026008430868387222
Time execution (tranning): 76.374992 seconds 
Time execution (load saved model): 0.001651 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 289
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.002758341059088707

num_try : 30 | val_loss = 0.002758341059088707 | val acc = 0.005476966034621
Time execution (tranning): 52.697762 seconds 
Time execution (load saved model): 0.001467 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 245
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002878267699852586

num_try : 14 | val_loss = 0.002878267699852586 | val acc = 0.005710482131689787
Time execution (tranning): 40.054358 seconds 
Time execution (load saved model): 0.001494 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 188
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0028986568097025154

num_try : 62 | val_loss = 0.0028986568097025154 | val acc = 0.005754747893661261
Time execution (tranning): 47.586612 seconds 
Time execution (load saved model): 0.001496 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 223
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.002936263745650649

num_try : 46 | val_loss = 0.002936263745650649 | val acc = 0.005827983841300011
Time execution (tranning): 42.069539 seconds 
Time execution (load saved model): 0.001512 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 200
Criterion: ExponentialLoss
with parameters: {'alpha': 0.5}
----------

0.0029463689681142567

num_try : 295 | val_loss = 0.0029463689681142567 | val acc = 0.0029213319066911936
Time execution (tranning): 73.298353 seconds 
Time execution (load saved model): 0.001540 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 313
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.002999104680493474

num_try : 263 | val_loss = 0.002999104680493474 | val acc = 0.002972173737362027
Time execution (tranning): 63.660801 seconds 
Time execution (load saved model): 0.001533 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 268
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0030309387110173704

num_try : 279 | val_loss = 0.0030309387110173704 | val acc = 0.0030036780517548323
Time execution (tranning): 78.101066 seconds 
Time execution (load saved model): 0.001561 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 331
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0030381191335618497

num_try : 311 | val_loss = 0.0030381191335618497 | val acc = 0.003010539337992668
Time execution (tranning): 61.543494 seconds 
Time execution (load saved model): 0.001582 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 253
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0031483025941997766

num_try : 247 | val_loss = 0.0031483025941997766 | val acc = 0.0031184712424874306
Time execution (tranning): 45.099825 seconds 
Time execution (load saved model): 0.001544 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 203
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003150817295536399

num_try : 199 | val_loss = 0.003150817295536399 | val acc = 0.003122624708339572
Time execution (tranning): 55.047614 seconds 
Time execution (load saved model): 0.001565 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 248
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0031528978142887354

num_try : 151 | val_loss = 0.0031528978142887354 | val acc = 0.0031184523832052946
Time execution (tranning): 38.862033 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 177
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0032508333027362825

num_try : 135 | val_loss = 0.0032508333027362825 | val acc = 0.003218489931896329
Time execution (tranning): 33.701821 seconds 
Time execution (load saved model): 0.001501 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 154
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0033102298248559236

num_try : 215 | val_loss = 0.0033102298248559236 | val acc = 0.003276232397183776
Time execution (tranning): 44.120792 seconds 
Time execution (load saved model): 0.001562 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 199
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0033629703987389805

num_try : 183 | val_loss = 0.0033629703987389805 | val acc = 0.003334204899147153
Time execution (tranning): 55.144761 seconds 
Time execution (load saved model): 0.001502 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 252
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0033706773258745672

num_try : 231 | val_loss = 0.0033706773258745672 | val acc = 0.003339947434142232
Time execution (tranning): 61.660904 seconds 
Time execution (load saved model): 0.001560 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 279
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003384295227006078

num_try : 383 | val_loss = 0.003384295227006078 | val acc = 0.0033552192617207766
Time execution (tranning): 86.170609 seconds 
Time execution (load saved model): 0.001573 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 329
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0034081149753183125

num_try : 87 | val_loss = 0.0034081149753183125 | val acc = 0.0033768096473068
Time execution (tranning): 50.736601 seconds 
Time execution (load saved model): 0.001478 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 238
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0034696756675839423

num_try : 71 | val_loss = 0.0034696756675839423 | val acc = 0.003436918370425701
Time execution (tranning): 53.307991 seconds 
Time execution (load saved model): 0.001489 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 250
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0035274683218449354

num_try : 119 | val_loss = 0.0035274683218449354 | val acc = 0.0034921562764793634
Time execution (tranning): 45.001465 seconds 
Time execution (load saved model): 0.001548 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 202
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0035554514080286025

num_try : 167 | val_loss = 0.0035554514080286025 | val acc = 0.0035206431057304144
Time execution (tranning): 66.085841 seconds 
Time execution (load saved model): 0.001511 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 306
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003561477055773139

num_try : 343 | val_loss = 0.003561477055773139 | val acc = 0.0035181106068193913
Time execution (tranning): 69.194924 seconds 
Time execution (load saved model): 0.001580 seconds 
Time execution (use saved model): 0.000170 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 249
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0035684752836823465

num_try : 103 | val_loss = 0.0035684752836823465 | val acc = 0.0035297188442200422
Time execution (tranning): 40.960351 seconds 
Time execution (load saved model): 0.001547 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 187
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0036008984316140413

num_try : 335 | val_loss = 0.0036008984316140413 | val acc = 0.003571326145902276
Time execution (tranning): 76.537636 seconds 
Time execution (load saved model): 0.001696 seconds 
Time execution (use saved model): 0.000181 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 297
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003733232030645013

num_try : 367 | val_loss = 0.003733232030645013 | val acc = 0.003699355060234666
Time execution (tranning): 68.135128 seconds 
Time execution (load saved model): 0.001590 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 267
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003744139475747943

num_try : 351 | val_loss = 0.003744139475747943 | val acc = 0.0037111020646989346
Time execution (tranning): 86.825365 seconds 
Time execution (load saved model): 0.001599 seconds 
Time execution (use saved model): 0.000172 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 330
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003920061085373163

num_try : 359 | val_loss = 0.003920061085373163 | val acc = 0.0038675679825246334
Time execution (tranning): 47.453002 seconds 
Time execution (load saved model): 0.001584 seconds 
Time execution (use saved model): 0.000171 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 182
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003923854632303119

num_try : 39 | val_loss = 0.003923854632303119 | val acc = 0.0038788130041211843
Time execution (tranning): 35.310081 seconds 
Time execution (load saved model): 0.001483 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 167
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.00392875119112432

num_try : 327 | val_loss = 0.00392875119112432 | val acc = 0.003874365473166108
Time execution (tranning): 50.381596 seconds 
Time execution (load saved model): 0.001606 seconds 
Time execution (use saved model): 0.000174 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[1024],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 192
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003930710516870022

num_try : 55 | val_loss = 0.003930710516870022 | val acc = 0.0038838020991533995
Time execution (tranning): 39.961947 seconds 
Time execution (load saved model): 0.001502 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 190
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003948474479839206

num_try : 23 | val_loss = 0.003948474479839206 | val acc = 0.003903537057340145
Time execution (tranning): 50.840088 seconds 
Time execution (load saved model): 0.001493 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 242
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003956852061673999

num_try : 319 | val_loss = 0.003956852061673999 | val acc = 0.003917500842362642
Time execution (tranning): 68.602163 seconds 
Time execution (load saved model): 0.001574 seconds 
Time execution (use saved model): 0.000169 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 274
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003963707536458969

num_try : 7 | val_loss = 0.003963707536458969 | val acc = 0.00391798373311758
Time execution (tranning): 35.863721 seconds 
Time execution (load saved model): 0.001460 seconds 
Time execution (use saved model): 0.000158 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 172
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.003989141127094626

num_try : 287 | val_loss = 0.003989141127094626 | val acc = 0.003949986770749092
Time execution (tranning): 46.571135 seconds 
Time execution (load saved model): 0.001565 seconds 
Time execution (use saved model): 0.000168 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 199
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004055917039513588

num_try : 271 | val_loss = 0.004055917039513588 | val acc = 0.0040116459131240845
Time execution (tranning): 38.649401 seconds 
Time execution (load saved model): 0.001534 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 163
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004143263194710017

num_try : 239 | val_loss = 0.004143263194710017 | val acc = 0.004105246160179377
Time execution (tranning): 61.442363 seconds 
Time execution (load saved model): 0.001524 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 277
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004164640568196773

num_try : 303 | val_loss = 0.004164640568196773 | val acc = 0.004122720565646887
Time execution (tranning): 56.528465 seconds 
Time execution (load saved model): 0.001539 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[512],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 245
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004208710128441453

num_try : 175 | val_loss = 0.004208710128441453 | val acc = 0.004166108556091785
Time execution (tranning): 45.011073 seconds 
Time execution (load saved model): 0.001532 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 209
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004220069805160165

num_try : 223 | val_loss = 0.004220069805160165 | val acc = 0.00418068515136838
Time execution (tranning): 82.849215 seconds 
Time execution (load saved model): 0.001543 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 371
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0042321972083300356

num_try : 159 | val_loss = 0.0042321972083300356 | val acc = 0.004189540632069111
Time execution (tranning): 46.425020 seconds 
Time execution (load saved model): 0.001517 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 211
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004318387219682336

num_try : 255 | val_loss = 0.004318387219682336 | val acc = 0.004274166189134121
Time execution (tranning): 58.951691 seconds 
Time execution (load saved model): 0.001554 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 267
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004453387130051851

num_try : 143 | val_loss = 0.004453387130051851 | val acc = 0.004413268994539976
Time execution (tranning): 61.218010 seconds 
Time execution (load saved model): 0.001503 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 276
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004454139862209558

num_try : 79 | val_loss = 0.004454139862209558 | val acc = 0.004403244704008102
Time execution (tranning): 54.465259 seconds 
Time execution (load saved model): 0.001514 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 257
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004499275973066688

num_try : 191 | val_loss = 0.004499275973066688 | val acc = 0.004455954302102327
Time execution (tranning): 45.413403 seconds 
Time execution (load saved model): 0.001562 seconds 
Time execution (use saved model): 0.000164 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[128],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 210
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.00460542444139719

num_try : 207 | val_loss = 0.00460542444139719 | val acc = 0.00455230288207531
Time execution (tranning): 46.515043 seconds 
Time execution (load saved model): 0.001965 seconds 
Time execution (use saved model): 0.000201 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[256],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 209
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004646057030186057

num_try : 95 | val_loss = 0.004646057030186057 | val acc = 0.004600348882377148
Time execution (tranning): 51.201951 seconds 
Time execution (load saved model): 0.001509 seconds 
Time execution (use saved model): 0.000165 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 235
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004922311324626207

num_try : 423 | val_loss = 0.004922311324626207 | val acc = 0.0048099844716489315
Time execution (tranning): 69.610765 seconds 
Time execution (load saved model): 0.001717 seconds 
Time execution (use saved model): 0.000184 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 229
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004940199563279748

num_try : 127 | val_loss = 0.004940199563279748 | val acc = 0.004871612414717674
Time execution (tranning): 39.361567 seconds 
Time execution (load saved model): 0.001504 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 184
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.004953414015471935

num_try : 111 | val_loss = 0.004953414015471935 | val acc = 0.004887832794338465
Time execution (tranning): 52.449529 seconds 
Time execution (load saved model): 0.001519 seconds 
Time execution (use saved model): 0.000166 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[64],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 246
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.005352320000529289

num_try : 431 | val_loss = 0.005352320000529289 | val acc = 0.0052174897864460945
Time execution (tranning): 197.501581 seconds 
Time execution (load saved model): 0.001637 seconds 
Time execution (use saved model): 0.000176 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 618
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.005509274192154408

num_try : 63 | val_loss = 0.005509274192154408 | val acc = 0.005427492782473564
Time execution (tranning): 53.522058 seconds 
Time execution (load saved model): 0.001472 seconds 
Time execution (use saved model): 0.000162 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 254
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.005559489205479622

num_try : 47 | val_loss = 0.005559489205479622 | val acc = 0.0054892003536224365
Time execution (tranning): 43.196617 seconds 
Time execution (load saved model): 0.001496 seconds 
Time execution (use saved model): 0.000161 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-07
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 205
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0056403780914843086

num_try : 31 | val_loss = 0.0056403780914843086 | val acc = 0.005558102857321501
Time execution (tranning): 55.484162 seconds 
Time execution (load saved model): 0.001492 seconds 
Time execution (use saved model): 0.000163 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 259
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0056606239825487134

num_try : 15 | val_loss = 0.0056606239825487134 | val acc = 0.005583140999078751
Time execution (tranning): 38.746488 seconds 
Time execution (load saved model): 0.001631 seconds 
Time execution (use saved model): 0.000167 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[32],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000002e-06
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 184
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0058244125917553905

num_try : 399 | val_loss = 0.0058244125917553905 | val acc = 0.005688192322850227
Time execution (tranning): 100.133389 seconds 
Time execution (load saved model): 0.001684 seconds 
Time execution (use saved model): 0.000180 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 313
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.005901692863553762

num_try : 391 | val_loss = 0.005901692863553762 | val acc = 0.005773786921054125
Time execution (tranning): 154.311172 seconds 
Time execution (load saved model): 0.001673 seconds 
Time execution (use saved model): 0.000178 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.01,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 470
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.0059443658962845805

num_try : 439 | val_loss = 0.0059443658962845805 | val acc = 0.005810076370835304
Time execution (tranning): 124.909136 seconds 
Time execution (load saved model): 0.001650 seconds 
Time execution (use saved model): 0.000177 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 401
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.006283166147768498

num_try : 415 | val_loss = 0.006283166147768498 | val acc = 0.006142113823443651
Time execution (tranning): 86.881478 seconds 
Time execution (load saved model): 0.001727 seconds 
Time execution (use saved model): 0.000185 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 283
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

0.007390244975686073

num_try : 407 | val_loss = 0.007390244975686073 | val acc = 0.007291742134839296
Time execution (tranning): 97.614819 seconds 
Time execution (load saved model): 0.001653 seconds 
Time execution (use saved model): 0.000175 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.01,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.0000000000000004e-08
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.2, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 316
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------

12613.08780517578

num_try : 447 | val_loss = 12613.08780517578 | val acc = 0.5248133540153503
Time execution (tranning): 16.523518 seconds 
Time execution (load saved model): 0.001670 seconds 
Time execution (use saved model): 0.000180 seconds 
Training with hyperparameters : ModelConfig : 
- model_name=Try Hyperparams,
- mode=Mode.DLMT_DQ,
- batch_size=64,
- n_nodes=[2048],
- activations=[GELU(approximate='none')],
- L1_penalty=0.001,
- L2_penalty=0.001,
- learning_rate=0.01,
- num_epochs=1000,
- optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
),
- criterion=ExponentialLoss(),
- p_dropout=0.5, 
- use_batch_norm=True,
- loss=0.0,
- mean_distance=0.0 

Num of epoch used : 52
Criterion: ExponentialLoss
with parameters: {'alpha': 1.0}
----------
